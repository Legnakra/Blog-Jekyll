<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-03-04T18:11:16+01:00</updated><id>/feed.xml</id><title type="html">sysmaria</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Cluster de Alta Disponibilidad</title><link href="/hlc+sri/2023/02/25/cluster.html" rel="alternate" type="text/html" title="Cluster de Alta Disponibilidad" /><published>2023-02-25T16:45:16+01:00</published><updated>2023-02-25T16:45:16+01:00</updated><id>/hlc+sri/2023/02/25/cluster</id><content type="html" xml:base="/hlc+sri/2023/02/25/cluster.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En este post vamos a ver instalar una aplicacioń php, en nuestro caso <code class="language-plaintext highlighter-rouge">WordPress</code>, sobre dos cluster de alta disponibilidad, uno con <code class="language-plaintext highlighter-rouge">Keepalived</code> y otro con <code class="language-plaintext highlighter-rouge">Pacemaker</code>.</p>

<p>Un cluster de alta disponibilidad es un conjunto de servidores que trabajan juntos para proporcionar un servicio de red, almacenamiento o aplicaciones. Los servidores de un cluster de alta disponibilidad se denominan nodos. Los nodos de un cluster de alta disponibilidad se configuran para que se comuniquen entre sí y para que trabajen juntos para proporcionar un servicio de red, almacenamiento o aplicaciones.</p>

<p>El escenario lo podemos encontrar en el siguiente <a href="https://github.com/josedom24/escenarios-HA/tree/master/07-HA-IPFailover-Apache2%2BDRBD%2BGFS2">repositorio</a>, donde nos encontramos el vagrantfile necesario para crear las tres máquinas virtuales necesarias para el escenario y el playbook de ansible para instalar el cluster.</p>

<h2 id="construimos-el-escenario">Construimos el escenario</h2>

<p>Para construir el escenario, nos situamos en el directorio donde tenemos el vagrantfile y ejecutamos:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vagrant up
</code></pre></div></div>

<p><img src="/assets/images/cluster/1.png" alt="1" /></p>

<p>Una vez terminado el proceso, podemos comprobar que tenemos las tres máquinas virtuales creadas:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vagrant status
</code></pre></div></div>

<p><img src="/assets/images/cluster/2.png" alt="2" /></p>

<p>Nos desplazamos al directorio donde tenemos el playbook de ansible y ejecutamos:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook <span class="nt">-b</span> site.yaml
</code></pre></div></div>

<p><img src="/assets/images/cluster/3.png" alt="3" /></p>

<h2 id="comenzamos">Comenzamos</h2>

<h3 id="cluster-de-ha-activo-pasivo">Cluster de HA activo-pasivo</h3>

<p>Tras levantar el escenario y aplicar la receta de ansible, podemos comprobar que se ha configurado correctamente el cluster y que los servicios se han instalado.</p>

<p>Para ello, nos conectamos al <code class="language-plaintext highlighter-rouge">nodo1</code> y comprobamos el estado del cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vagrant ssh nodo1
<span class="nb">sudo </span>su
pcs status
</code></pre></div></div>

<p><img src="/assets/images/cluster/4.png" alt="4" /></p>

<p>El siguiente paso es comprobar que podemos acceder a la web a través del navegador, deberemos cambiar la configuración de nuestra máquina host para que emplee la IP del <code class="language-plaintext highlighter-rouge">nodo1</code> como servidor DNS, para ello, en el fichero <code class="language-plaintext highlighter-rouge">/etc/resolv.conf</code> añadimos la siguiente línea:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nameserver 192.168.121.48
</code></pre></div></div>

<p><img src="/assets/images/cluster/5.png" alt="5" /></p>

<p>Si nos dirigimos a la url <a href="www.example.com/index.php">www.example.com/index.php</a>, podemos ver que podemos acceder a la web y en el <code class="language-plaintext highlighter-rouge">index.php</code> podemos ver que efectivamente, está funcionando con el nodo1.</p>

<p><img src="/assets/images/cluster/6.png" alt="6" /></p>

<p>En caso que el nodo1 se apague, la web funcionará sobre el nodo2. Por ello, apagaremos de forma manual el nodo1.</p>

<p><img src="/assets/images/cluster/7.png" alt="7" /></p>

<p>Y cuando accedamos al <code class="language-plaintext highlighter-rouge">index.php</code>, y como podemos comprobar en la siguiente imagen, el servidor web está activo gracias al <code class="language-plaintext highlighter-rouge">nodo2</code>.</p>

<p><img src="/assets/images/cluster/8.png" alt="8" /></p>

<h4 id="mariadb-galera-cluster">MariaDB Galera Cluster</h4>

<p>Ahora instalaremos en los dos nodos un <strong>Galera MariaDB</strong> para emplearlo como base de datos en alta disponibilidad y que usará nuestro Wordpress, para ello, ejecutamos en ambos nodos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt <span class="nb">install </span>mariadb-server
</code></pre></div></div>

<p><img src="/assets/images/cluster/9.png" alt="9" /></p>

<p>Para tener una base de datos segura, vamos a securizar mariadb de la siguiente manera:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mysql_secure_installation
</code></pre></div></div>

<p>Tras dejar a punto el gestor de base de datos, vamos a elegir un nodo, que en nuestro caso será el nodo 1, por lo que detenemos el servicio de mariadb:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl stop mariadb.service
</code></pre></div></div>

<p><img src="/assets/images/cluster/10.png" alt="10" />]</p>

<p>Y modificamos el fichero del cluster Galera de la siguiente manera:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano /etc/mysql/mariadb.conf.d/60-galera.cnf
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="o">[</span>galera]
<span class="c">#Activa el cluster</span>
wsrep_on                 <span class="o">=</span> ON
<span class="c">#Nombre del cluster                            </span>
wsrep_cluster_name       <span class="o">=</span> <span class="s2">"MariaDB Galera Cluster"</span>
<span class="c">#Ruta al módulo de Galera     </span>
wsrep_provider           <span class="o">=</span> /usr/lib/galera/libgalera_smm.so
<span class="c">#Dirección IP de los nodos participantes en el cluster </span>
wsrep_cluster_address    <span class="o">=</span> gcomm://10.1.1.101,10.1.1.102
<span class="c">#Formato de binlog</span>
binlog_format            <span class="o">=</span> row
<span class="c">#Motor de almacenamiento</span>
default_storage_engine   <span class="o">=</span> InnoDB
<span class="c">#Tamaño del buffer de memoria</span>
innodb_autoinc_lock_mode <span class="o">=</span> 2

<span class="c"># Allow server to accept connections on all interfaces.</span>
bind-address <span class="o">=</span> 0.0.0.0
<span class="c">#Dirección IP del nodo cluster de la base de datos</span>
<span class="nv">wsrep_node_address</span><span class="o">=</span>10.1.1.101
</code></pre></div></div>

<p><img src="/assets/images/cluster/11.png" alt="11" /></p>

<p>Creamos el cluster de la siguiente manera:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>galera_new_cluster
</code></pre></div></div>

<p>Y arrancamos el servicio de mariadb:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl start mariadb.service
</code></pre></div></div>

<p><img src="/assets/images/cluster/12.png" alt="12" /></p>

<p>Para comprobar que el cluster está funcionando correctamente, ejecutamos el siguiente comando dentro de la base de datos:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mysql <span class="nt">-u</span> root <span class="nt">-p</span> <span class="nt">-e</span> <span class="s2">"SHOW STATUS LIKE 'wsrep_cluster_size'"</span>
</code></pre></div></div>

<p><img src="/assets/images/cluster/13.png" alt="13" /></p>

<p>Como podemos ver en la imagen anterior, el cluster está formado por un nodo, por lo que ahora vamos a añadir el nodo2 al cluster. Realizamos los mismos pasos que en el nodo1, pero en este caso, cambiamos la dirección IP del nodo2 en el fichero de configuración del cluster.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>galera]
<span class="c">#Activa el cluster</span>
wsrep_on                 <span class="o">=</span> 1
<span class="c">#Nombre del cluster </span>
wsrep_cluster_name       <span class="o">=</span> <span class="s2">"MariaDB Galera Cluster"</span>
<span class="c">#Ruta al módulo de Galera</span>
wsrep_provider           <span class="o">=</span> /usr/lib/galera/libgalera_smm.so
<span class="c">#Dirección IP de los nodos participantes en el cluster </span>
wsrep_cluster_address    <span class="o">=</span> gcomm://10.1.1.101,10.1.1.102
binlog_format            <span class="o">=</span> row
<span class="c">#Motor de almacenamiento</span>
default_storage_engine   <span class="o">=</span> InnoDB
<span class="c">#Tamaño del buffer de memoria</span>
innodb_autoinc_lock_mode <span class="o">=</span> 2

<span class="c"># Allow server to accept connections on all interfaces.</span>
bind-address <span class="o">=</span> 0.0.0.0
<span class="c">#Dirección IP del nodo cluster de la base de datos</span>
<span class="nv">wsrep_node_address</span><span class="o">=</span>10.1.1.102
</code></pre></div></div>

<p><img src="/assets/images/cluster/14.png" alt="14" /></p>

<p>Y arrancamos el servicio de mariadb:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl start mariadb.service
</code></pre></div></div>

<p><img src="/assets/images/cluster/15.png" alt="15" /></p>

<p>Ahora, ejecutamos de nuevo el comando <code class="language-plaintext highlighter-rouge">mysql -u root -p -e "SHOW STATUS LIKE 'wsrep_cluster_size'"</code> para comprobar que el cluster está formado por dos nodos.</p>

<p><img src="/assets/images/cluster/16.png" alt="16" /></p>

<p>Ya comprobado que nuestro cluster de mariadb Galera funciona correctamente, vamos a crear una base de datos y un usuario para que el wordpress pueda acceder a ella.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE DATABASE wordpress<span class="p">;</span>
GRANT ALL ON wordpress.<span class="k">*</span> TO <span class="s1">'wordpress'</span>@<span class="s1">'%'</span> IDENTIFIED BY <span class="s1">'wordpress'</span><span class="p">;</span>
</code></pre></div></div>

<p><img src="/assets/images/cluster/17.png" alt="17" /></p>

<h4 id="wordpress">Wordpress</h4>

<p>Ahora le toca el turno a wordpres, por lo que nos lo vamos a descargar en el nodo 1. Navegamos a la carpeta <code class="language-plaintext highlighter-rouge">/var/www/html</code> y ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://es.wordpress.org/latest-es_ES.tar.gz
</code></pre></div></div>

<p><img src="/assets/images/cluster/18.png" alt="18" /></p>

<p>Lo descomprimimos, y le cambiamos el propietario a <code class="language-plaintext highlighter-rouge">www-data</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar</span> <span class="nt">-xvzf</span> latest-es_ES.tar.gz
<span class="nb">chown</span> <span class="nt">-R</span> www-data: wordpress/
</code></pre></div></div>

<p><img src="/assets/images/cluster/19.png" alt="19" /></p>

<p>Accedemos a <a href="https://www.example.com/wordress">https://www.example.com/wordress</a> e instalamos wordpress:</p>

<p><img src="/assets/images/cluster/20.png" alt="20" /></p>

<p>Como podemos ver en la imagen anterior, está instalado y le hemos añadido una nueva entrada.</p>

<h4 id="comprobar-alta-disponibilidad">Comprobar Alta disponibilidad</h4>

<p>Si apagásemos el nodo1, todos los recursos de este nodo deberían de pasar al nodo2, y por ello vamos a comprobarlo:</p>

<ul>
  <li>
    <p>Apagamos nodo1 y verificamos en el nodo2 el estado del cluster:</p>

    <p><img src="/assets/images/cluster/21.png" alt="21" /></p>
  </li>
</ul>

<p>Como podemos ver, se hemos podido acceder a nuestra nueva entrada y por ello, podemos verificar, que la configuración de nuestra cluster en alta disponibilidad, con una aplicación php y una base de datos, funciona correctamente.</p>

<p><img src="/assets/images/cluster/22.png" alt="22" /></p>

<h3 id="cluster-de-ha-activo-activo">Cluster de HA activo-activo</h3>

<p>Siguiendo las instrucciones que encontraremos en el <a href="https://github.com/josedom24/escenarios-HA/tree/master/07-HA-IPFailover-Apache2+DRBD+GFS2">escenario 7</a>, convertiremos el clúster en activo-activo. Vamos a instalar el <a href="https://github.com/josedom24/escenarios-HA/blob/master/07-HA-IPFailover-Apache2%2BDRBD%2BGFS2/fencing.md">fencing</a> para que el clúster funcione de manera adecuada.</p>

<p>Partiendo del ejercicio anterior, donde hemos configurado un cluster de <strong>IP Failover + Apache2 + DRBD</strong>, vamos a agregar un sistema de almacenamiento distribuido <strong>GFS2</strong>, por lo que podremos configurar nuestros sistema <strong>DRBD</strong> como Dual-primary, y la IP ClusterIP podrá estar asignado a cualquiera de los nodos, ya que los nodos podrán escribir o leer al mismo tiempo.</p>

<p>En definitiva, vamos a convertir nuestro clúster en activo-activo.</p>

<h4 id="instalación-de-gfs2">Instalación de GFS2</h4>

<p>En este apartado vamos a configurar GFS2 como sistema de almacenamiento distribuido para conseguir el cluster de alta disponibilidad activo-activo.</p>

<p>Además vamos a instalar el programa DLM (Distributed Lock Manager) que será el encargado de gestionar el acceso del clúster al almacenamiento distribuido.</p>

<p>En los dos nodos, instalamos el paquete <code class="language-plaintext highlighter-rouge">gfs2-utils</code> y <code class="language-plaintext highlighter-rouge">dlm</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt-get <span class="nb">install </span>gfs2-utils dlm-controld
</code></pre></div></div>

<p><img src="/assets/images/cluster/23.png" alt="23" /></p>

<p>El DLM se tiene que ejecutar en los dos nodos, vamos a crear un recurso <strong>ocf:pacemaker:controld</strong> y lo vamos a clonar. Para ello, nos dirigimos a la consola del nodo1 y ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs cluster cib dlm_cfg
pcs <span class="nt">-f</span> dlm_cfg resource create dlm ocf:pacemaker:controld op monitor <span class="nv">interval</span><span class="o">=</span>60s
pcs <span class="nt">-f</span> dlm_cfg resource clone dlm clone-max<span class="o">=</span>2 clone-node-max<span class="o">=</span>1
pcs cluster cib-push dlm_cfg <span class="nt">--config</span>
</code></pre></div></div>

<p><img src="/assets/images/cluster/24.png" alt="24" /></p>

<p>Comrpobamos que el recurso se ha creado correctamente:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs status
</code></pre></div></div>

<p><img src="/assets/images/cluster/25.png" alt="25" /></p>

<h4 id="creamos-el-sistema-de-archivos-gfs2">Creamos el sistema de archivos GFS2</h4>

<p>Antes de continuar, vamos a deshabilitar el recurso que controlaba el sistema de archivo del ejercicio anterior:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs resource disable WebFS
</code></pre></div></div>

<p>Y veremos como los recursos <code class="language-plaintext highlighter-rouge">WebFS</code> y <code class="language-plaintext highlighter-rouge">WebSite</code> se han detenido:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs status
</code></pre></div></div>

<p><img src="/assets/images/cluster/26.png" alt="26" /></p>

<p>El siguiente paso será formatear el dispositivo de bloques del nodo1 con el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkfs.gfs2 <span class="nt">-p</span> lock_dlm <span class="nt">-j</span> 2 <span class="nt">-t</span> mycluster:web /dev/drbd1
</code></pre></div></div>

<p><img src="/assets/images/cluster/27.png" alt="27" /></p>

<p>Las opciones que hemos utilizado son:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-p lock_dlm</code>: Indica que vamos a usar el programa DLM (Distributed Lock Manager) para gestionar los cambiso del sistema de archivo.</li>
  <li><code class="language-plaintext highlighter-rouge">-j 2</code>: Se va a reservar espacio para 2 journals (registro donde se almacena información necesaria para recuperar los datos afectados por una transición en caso de que falle) uno para cada nodo.</li>
  <li><code class="language-plaintext highlighter-rouge">-t mycluster:web</code>: El nombre de la tabla de bloqueo (lock) (<code class="language-plaintext highlighter-rouge">web</code>) en el cluster <code class="language-plaintext highlighter-rouge">mycluster</code> (nombre del cluster que indicamos al crearlo con corosync y que lo podemos encontrar en <code class="language-plaintext highlighter-rouge">/etc/corosync/corosync.conf</code>).</li>
</ul>

<p>A continuación, vamos a guardar la información en el dispositivo de bloques. Creamos el fichero <code class="language-plaintext highlighter-rouge">index.html</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mount /dev/drbd1 /mnt
<span class="nb">cd</span> /mnt
<span class="nb">echo</span> <span class="s2">"&lt;h1&gt;Prueba con GFS2&lt;/h1&gt;"</span> <span class="o">&gt;&gt;</span> index.html
<span class="nb">cd
</span>umount /mnt
</code></pre></div></div>

<p><img src="/assets/images/cluster/28.png" alt="28" /></p>

<p>Una vez terminado esto, vamos a reconfigurar el recurso <code class="language-plaintext highlighter-rouge">WebFS</code> del cluster con el nuevo sistema de fichero que hemos configurado:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs resource update WebFS <span class="nv">fstype</span><span class="o">=</span>gfs2
</code></pre></div></div>

<p>Y como va a necesitr que DLM esté activado, tenemos que añadir dos restricciones: la primera es para que el recurso <code class="language-plaintext highlighter-rouge">WebFS</code> se ejecute en el mismo nodo que el recurso <code class="language-plaintext highlighter-rouge">dlm-clone</code>, y la segunda es para que el recurso <code class="language-plaintext highlighter-rouge">dlm-clone</code> se ejecute antes que el recurso <code class="language-plaintext highlighter-rouge">WebFS</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs constraint colocation add WebFS with dlm-clone INFINITY
pcs constraint order dlm-clone <span class="k">then </span>WebFS
</code></pre></div></div>

<p><img src="/assets/images/cluster/29.png" alt="29" /></p>

<p>El último paso que nos queda para terminar de configurar el sistema de archivos GFS2, es montar el sistema de archivos en los dos nodos y reconfigurar el recurso <code class="language-plaintext highlighter-rouge">WebData-clone</code> indicando que se ejecuten como primario en el DRBD.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs cluster cib active_cfg
pcs <span class="nt">-f</span> active_cfg resource clone WebFS
pcs <span class="nt">-f</span> active_cfg constraint
pcs <span class="nt">-f</span> active_cfg resource update WebData-clone promoted-max<span class="o">=</span>2
pcs cluster cib-push active_cfg <span class="nt">--config</span>
pcs resource <span class="nb">enable </span>WebFS
</code></pre></div></div>

<p><img src="/assets/images/cluster/30.png" alt="30" /></p>

<p>En este momento tenemos el DRBD como dual-primary y el sistema de ficheros GFS2 montado en los dos nodos. Cualquiera de los servidores web pueden escribir ficheros en /var/www/html, por lo que podemos clonar el recurso WebSite y quitar la restricción de colocación que hacía que el servidor web se activa en el nodo que tenía asignada la VirtualIP. Para ello:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs cluster cib active_cfg
pcs <span class="nt">-f</span> active_cfg resource clone WebSite
pcs cluster cib-push active_cfg <span class="nt">--config</span>
pcs constraint colocation delete WebSite-clone VirtualIP
</code></pre></div></div>

<p><img src="/assets/images/cluster/31.png" alt="31" /></p>

<p>Si realizamos un <code class="language-plaintext highlighter-rouge">pcs status</code> veremos que el recurso <code class="language-plaintext highlighter-rouge">WebSite</code> se ha clonado y que la IP ClusterIP se ha asignado a ambos nodos, y podemos ver que está activo:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs status
lsblk <span class="nt">-f</span>
</code></pre></div></div>

<p><img src="/assets/images/cluster/32.png" alt="32" /></p>

<p><img src="/assets/images/cluster/33.png" alt="33" /></p>

<p>Ahora, solo queda configurar el <code class="language-plaintext highlighter-rouge">Fencing</code> para que en caso de fallo de un nodo, se pueda recuperar el servicio, y el <code class="language-plaintext highlighter-rouge">STONITH</code> para que en caso de fallo de un nodo, se pueda recuperar el servicio y que el nodo que ha fallado se pueda recuperar. Para ver los agentes que tenemos disponibles, ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs stonith list
</code></pre></div></div>
<p><img src="/assets/images/cluster/34.png" alt="34" /></p>

<p>Al estar usando <em>KVM</em> como hipervisor, vamos a hacer uso del <code class="language-plaintext highlighter-rouge">external/libvirt</code> para el <code class="language-plaintext highlighter-rouge">STONITH</code>. Para ello, vamos a instalar el paquete <code class="language-plaintext highlighter-rouge">libvirt-clients</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt-get <span class="nb">install </span>libvirt-clients
</code></pre></div></div>

<p><img src="/assets/images/cluster/35.png" alt="35" /></p>

<p>Ambos nodos deberán de ser capaces de acceder al host a través de SSH, generaremos el par de claves en ambos nodos y las añadimos al fichero de authorized_keys del host:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-t</span> rsa
ssh-copy-id 192.168.121.1
</code></pre></div></div>

<p><img src="/assets/images/cluster/36.png" alt="36" /></p>

<p>Comprobamos que los parámetros que necesitamos para configurar el <code class="language-plaintext highlighter-rouge">STONITH</code> están bien:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs stonith describe external/libvirt
</code></pre></div></div>

<p>Como vemos en la siguiente imagen, hemos de indicar dos parámetros de forma obligatoria:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">hostlist</code>: Lista de nodos que se van a utilizar para el <code class="language-plaintext highlighter-rouge">STONITH</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">hostlist</span><span class="o">=</span><span class="s2">"nodo1:06-HA-IPFailover-Apache2DRBDGFS2_nodo1,nodo2:06-HA-IPFailover-Apache2DRBDGFS2_nodo2"</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">hypervisor_uri</code>: URI del servidor KVM.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nv">hypervisor_uri</span><span class="o">=</span><span class="s2">"qemu+ssh://192.168.121.1/system"</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Ya tenemos todo configurado para poder hacer uso del <code class="language-plaintext highlighter-rouge">STONITH</code>. Solo nos resta habilitar el <code class="language-plaintext highlighter-rouge">Fencing</code> en el cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pcs cluster cib stonith_cfg
pcs <span class="nt">-f</span> stonith_cfg stonith create fencing-libvirt external/libvirt <span class="se">\</span>
 <span class="nv">hostlist</span><span class="o">=</span><span class="s2">"nodo1:06-HA-IPFailover-Apache2DRBDGFS2_nodo1,nodo2:06-HA-IPFailover-Apache2DRBDGFS2_nodo2"</span> <span class="se">\</span>
 <span class="nv">hypervisor_uri</span><span class="o">=</span><span class="s2">"qemu+ssh://192.168.121.1/system"</span>
pcs <span class="nt">-f</span> stonith_cfg property <span class="nb">set </span>stonith-enabled<span class="o">=</span><span class="nb">true
</span>pcs cluster cib-push stonith_cfg <span class="nt">--config</span>
</code></pre></div></div>

<p><img src="/assets/images/cluster/37.png" alt="37" /></p>

<h4 id="prueba-de-funcionamiento">Prueba de funcionamiento</h4>

<p>Para poder realizar las pruebas de funcionamiento del cluster, vamos a volver a instalar Wordpress, dado que al formatear el dispositivo de bloques hemos perdido toda la información que teníamos en el sistema de ficheros. Sólo debemos realizar los pasos que hemos realizado en el apartado <a href="####Wordpress">Instalación de Wordpress</a>. La información no es necesario recuperarla porque está guardada en la base de datos, por lo que solo debemos configurar de nuevo las variables de la base de datos que ya teníamos de antemano.</p>

<p>Una vez instalado Wordpress, accederemos a la página y veremos que el post sigue existiendo. La imagen he tenido que añadirla de nuevo, dado que no se había guardado en la base de datos, pero el post existía.</p>

<p><img src="/assets/images/cluster/38.png" alt="38" /></p>

<h4 id="balanceador-de-carga">Balanceador de carga</h4>

<p>Para que el balanceador de carga, vamos a instalar un <em>HAproxy</em> en el nodo dns y lo configuraremos de la siguiente forma:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt <span class="nb">install </span>haproxy
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano /etc/haproxy/haproxy.cfg
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>frontend servidores_web
        <span class="nb">bind</span> <span class="k">*</span>:80
        mode http
        stats <span class="nb">enable
        </span>stats uri /ha_stats
        stats auth  cda:cda
        default_backend servidores_web_backend

backend servidores_web_backend
        mode http
        balance roundrobin
        server backend1 10.1.1.101:80 check
        server backend2 10.1.1.102:80 check
</code></pre></div></div>

<p><img src="/assets/images/cluster/39.png" alt="39" /></p>

<p>Realizado esto, también modificaremos las zonas dns, y para ello, debemos modificar los siguientes ficheros de configuración de las zonas dns:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano /var/cache/bind/db.10.1.1

<span class="c">###</span>

103     PTR     www.example.com
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano /var/cache/bind/db.example.com

<span class="c">###</span>

www     PTR     10.1.1.103
</code></pre></div></div>

<p>Tras estas modificaciones, deberemos reiniciar el servicio <code class="language-plaintext highlighter-rouge">bind9</code> para que se apliquen los cambios:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl restart bind9
</code></pre></div></div>

<p>Solo nos queda comprobar que podemos acceder a la web:</p>

<p><img src="/assets/images/cluster/40.png" alt="40" /></p>]]></content><author><name></name></author><category term="HLC+SRI" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Escenario - Poblar un directorio LDAP empleando certificado x509</title><link href="/aso/2023/02/21/ldaps.html" rel="alternate" type="text/html" title="Escenario - Poblar un directorio LDAP empleando certificado x509" /><published>2023-02-21T16:45:16+01:00</published><updated>2023-02-21T16:45:16+01:00</updated><id>/aso/2023/02/21/ldaps</id><content type="html" xml:base="/aso/2023/02/21/ldaps.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En nuestro <a href="https://sysmaria.netlify.app/hlc+sri/2022/12/05/escenario.html">escenario</a> vamos a realizar la configuración del servidor LDAP para que se comunique con el servidor de autenticación mediante certificados x509.</p>

<h2 id="creamos-el-certificado-x509">Creamos el certificado x509</h2>

<p>Como parte principal y esencial de esta práctica, vamos a crear un certificado x509 para el servidor LDAP. Para ello, vamos a utilizar el comando <code class="language-plaintext highlighter-rouge">openssl</code> que nos permite generar certificados x509. Para ello ejecutaremos el siguiente comando:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```bash
openssl genrsa 4096 &gt; /etc/ssl/private/alfa.key
```

Una vez ejecutado, debemos introducir los datos del certificado. Para ello, introduciremos los siguientes datos:

```bash
Country Name (2 letter code) [AU]:ES
State or Province Name (full name) [Some-State]:Sevilla
Locality Name (eg, city) []:Dos Hermanas
Organization Name (eg, company) [Internet Widgits Pty Ltd]:IES Gonzalo Nazareno
Organizational Unit Name (eg, section) []:Informática
Common Name (e.g. server FQDN or YOUR name) []:alfa.mariajesus.gonzalonazareno.org
```
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap3/1.png" alt="1" /></p>

<p>Ya generado el fichero, lo pasamos a nuestra máquina host por medio de <code class="language-plaintext highlighter-rouge">scp</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```bash
scp alfa.csr maria@172.22.2.125:/home/maria
```
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap3/2.png" alt="2" /></p>

<p>Lo mandamos a la autoridad certificadora para que nos lo firme y, de ser que lo hayamos generado correctamente, nos devolverá un certificado firmado. Lo descargamos y lo copiamos en la máquina <code class="language-plaintext highlighter-rouge">Alfa</code>:</p>

<p><img src="/assets/images/LDAP/ldap3/3.png" alt="3" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```bash
scp alfa.crt alfa:172.22.201.46:/home/maria
```
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap3/4.png" alt="4" /></p>

<p>Como podemos ver, el certificado ya está firmado y listo para ser utilizado.</p>

<h2 id="configuración-del-servidor-ldap">Configuración del servidor LDAP</h2>

<p>Una vez que tenemos el certificado x509, vamos a configurar el servidor LDAP para que se comunique con el servidor de autenticación mediante certificados x509. Para ello, vamos a realizar los siguientes pasos:</p>

<ol>
  <li>Modificar el fichero <code class="language-plaintext highlighter-rouge">/etc/ldap/ldap.conf</code> para que el servidor LDAP se comunique con el servidor de autenticación mediante certificados x509.</li>
</ol>]]></content><author><name></name></author><category term="ASO" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Taller: Corrector ortográfico de documentos markdown (test)</title><link href="/iaw/2023/02/19/jenkins1.html" rel="alternate" type="text/html" title="Taller: Corrector ortográfico de documentos markdown (test)" /><published>2023-02-19T16:45:16+01:00</published><updated>2023-02-19T16:45:16+01:00</updated><id>/iaw/2023/02/19/jenkins1</id><content type="html" xml:base="/iaw/2023/02/19/jenkins1.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>maginemos que estamos escribiendo documentos markdown y lo guardamos en un repositorio de github. Queremos que cada vez que hagamos una modificación (commit - push) se pruebe (test) de forma automática si tienes alguna falta de ortografía. Ese proceso lo vamos a hacer de forma automática y continua con Jenkins. Recuerda que el repositorio es https://github.com/josedom24/ic-diccionario</p>

<h2 id="definición-del-pipeline">Definición del pipeline</h2>

<p>Hasta ahora al definir un pipeline lo hemos escrito directamente en la configuración. Otra forma de hacerlo es tener definido el pipeline en un fichero llamado Jenkinsfile que estará en un repositorio.</p>

<ol>
  <li>
    <p>Vamos a crear un contenedor con debian, y vamos a usar el usuario root en la imagen.</p>
  </li>
  <li>
    <p>Instalamos git y nano</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> FROM debian:latest
 RUN apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> git nano
 USER root
</code></pre></div>    </div>
  </li>
  <li>
    <p>Clonamos el repositorio con el comando <code class="language-plaintext highlighter-rouge">git</code> en el stage <code class="language-plaintext highlighter-rouge">clone</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> pipeline <span class="o">{</span>
 agent any
 stages <span class="o">{</span>
     stage<span class="o">(</span><span class="s1">'clone'</span><span class="o">)</span> <span class="o">{</span>
         steps <span class="o">{</span>
             sh <span class="s1">'git clone https://github.com/legnakra/ic-diccionario'</span>
         <span class="o">}</span>
     <span class="o">}</span>
 <span class="o">}</span>
 <span class="o">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>En el stage Install actualizamos e instalamos la herramienta que vamos a utilizar. Podríamos partir de una imagen construida por nosotros donde tuviéramos ya esta herramienta instalada, y no haría falta este paso.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> stage<span class="o">(</span><span class="s1">'Install'</span><span class="o">)</span> <span class="o">{</span>
     steps <span class="o">{</span>
         sh <span class="s1">'apt-get update'</span>
         sh <span class="s1">'apt-get install -y aspell aspell-es'</span>
     <span class="o">}</span>
 <span class="o">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>En el stage Test hacemos la comprobación, hemos configurado el contenedor para que use UTF8.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> stage<span class="o">(</span><span class="s1">'Test'</span><span class="o">)</span> <span class="o">{</span>
     steps <span class="o">{</span>
         sh <span class="s1">'export LANG=es_ES.UTF-8'</span>
         sh <span class="s1">'aspell --lang=es --mode=tex check ic-diccionario/README.md'</span>
     <span class="o">}</span>
 <span class="o">}</span>
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="disparador-del-pipeline">Disparador del pipeline</h2>

<p>Tenemos varias formas de activar de forma automática la ejecución del pipeline:</p>

<p>En este ejercicio vamos a usar la opción Consultar repositorio (SCM). Esta opción no es exactamente cuando se hace un push en el repositorio, sino que se pone un programador cron indicando cada cuanto tiempo se mira el repositorio, si ha cambiado el repositorio se lanza el pipeline. Nosotros vamos a poner: * * * * *, miraremos el repositorio cada minuto:</p>

<p>IMAGEN</p>

<h2 id="notificaciones-del-pipeline">Notificaciones del pipeline</h2>

<p>Vamos a aprender como podemos hacer que el pipeline mande un correo al finalizar. Para ello lo primero instala un servidor de correo de forma adecuada en la máquina que tiene Jenkins. Lo siguiente es configurar Jenkins en Administrar Jenkins -&gt; Configurar el sistema:</p>

<p>IMAGEN</p>

<p>Y al pileline le añadimos las siguientes líneas:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    stages <span class="o">{</span>
    ...
    <span class="o">}</span>
    post <span class="o">{</span>
         always <span class="o">{</span>
          mail to: <span class="s1">'usuario@example.com'</span>,
          subject: <span class="s2">"Status of pipeline: </span><span class="k">${</span><span class="nv">currentBuild</span><span class="p">.fullDisplayName</span><span class="k">}</span><span class="s2">"</span>,
          body: <span class="s2">"</span><span class="k">${</span><span class="nv">env</span><span class="p">.BUILD_URL</span><span class="k">}</span><span class="s2"> has result </span><span class="k">${</span><span class="nv">currentBuild</span><span class="p">.result</span><span class="k">}</span><span class="s2">"</span>
        <span class="o">}</span>
      <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="IAW" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Introducción a los Pipelines de Jenkins</title><link href="/iaw/2023/02/17/jenkins-pipelines.html" rel="alternate" type="text/html" title="Introducción a los Pipelines de Jenkins" /><published>2023-02-17T18:45:16+01:00</published><updated>2023-02-17T18:45:16+01:00</updated><id>/iaw/2023/02/17/jenkins-pipelines</id><content type="html" xml:base="/iaw/2023/02/17/jenkins-pipelines.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En Jenkins se pueden realizar distintos tipos de tareas automatizadas. Pero nosotros vamos a usar los Pipelines.</p>

<p>Un Pipeline es una secuencia de tareas automatizadas que definen el ciclo de vida de la aplicación de nuestro flujo de integración/entrega/despliegue continuo. Podemos decir que un Pipeline es un conjunto de instrucciones del proceso que siga una aplicación desde el repositorio de control de versiones hasta que llega a los usuarios.</p>

<p><img src="/assets/images/jenkins/pipelines/1.png" alt="1" /></p>

<ul>
  <li>
    <p><strong>Disparadores</strong>: Motivo por el cual se comienza la ejecución de tareas automáticas. Puede ser por varios motivos: push en un repositorio github, ejecución cada cierto tiempo, finalización de otra tarea,…</p>
  </li>
  <li>
    <p><strong>Stage</strong>: Son las etapas lógicas en las que se dividen los flujos de trabajo de Jenkins. Es una práctica recomendada dividir nuestro flujo de trabajo en etapas ya que nos ayudará a organizar nuestros pipelines en fases. Ejemplos de fases: build, test, deploy,…</p>
  </li>
  <li>
    <p><strong>Steps</strong>: Son las tareas ó comandos que ejecutados de forma secuencial implementan la lógica de nuestro flujo de trabajo.</p>
  </li>
  <li>
    <p><strong>Node</strong>: Máquina que es parte del entorno de Jenkins y es capaz de ejecutar un Pipeline Jenkins. También llamada agentes de ejecución. Pueden ser la misma máquina donde tenemos instalado Jenkins, o máquinas configuradas para este fin. También podemos usar contenedores docker como agentes de ejecución. Es importante reseñar que el directorio de trabajo (workspace) es compartido por los steps del nodo, de forma que steps de un nodo pueden acceder a ficheros/directorios generados por steps de ese mismo nodo.</p>
  </li>
  <li>
    <p><strong>Notificaciones</strong>: Por ejemplo que envíe un correo electrónico al terminar.</p>
  </li>
</ul>

<h2 id="creando-pipelines">Creando Pipelines</h2>

<ol>
  <li>
    <p>Creamos una Nueva Tarea, y le ponemos un nombre y elegimos el tipo Pipeline:</p>

    <p><img src="/assets/images/jenkins/pipelines/2.png" alt="2" /></p>
  </li>
  <li>
    <p>En el apartado Pipeline, escribimos nuestro primer pipeline:</p>

    <div class="language-groovy highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">pipeline</span> <span class="o">{</span>
     <span class="n">agent</span> <span class="n">any</span>
     <span class="n">stages</span> <span class="o">{</span>
         <span class="n">stage</span><span class="o">(</span><span class="s1">'Build'</span><span class="o">)</span> <span class="o">{</span>
             <span class="n">steps</span> <span class="o">{</span>
                 <span class="n">echo</span> <span class="s1">'Tareas para construir, instalar,...'</span>
             <span class="o">}</span>
         <span class="o">}</span>
         <span class="n">stage</span><span class="o">(</span><span class="s1">'Test'</span><span class="o">)</span> <span class="o">{</span>
             <span class="n">steps</span> <span class="o">{</span>
                 <span class="n">echo</span> <span class="s1">'Tareas para realizar test.'</span>
             <span class="o">}</span>
         <span class="o">}</span>
         <span class="n">stage</span><span class="o">(</span><span class="s1">'Deploy'</span><span class="o">)</span> <span class="o">{</span>
             <span class="n">steps</span> <span class="o">{</span>
                 <span class="n">echo</span> <span class="s1">'Tareas para desplegar, construir, ...'</span>
             <span class="o">}</span>
         <span class="o">}</span>
     <span class="o">}</span>
 <span class="o">}</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/jenkins/pipelines/3.png" alt="3" /></p>
  </li>
  <li>
    <p>Le damos a Guardar y ya podemos Construir ahora para ejecutar el pipeline y construir un Build:</p>

    <p><img src="/assets/images/jenkins/pipelines/4.png" alt="4" /></p>
  </li>
  <li>
    <p>Y si vemos la Console Output vemos la salida del build:</p>

    <p><img src="/assets/images/jenkins/pipelines/5.png" alt="5" /></p>
  </li>
</ol>]]></content><author><name></name></author><category term="IAW" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Instalación de Jenkins en Debian 11</title><link href="/iaw/2023/02/17/jenkins-instalacion.html" rel="alternate" type="text/html" title="Instalación de Jenkins en Debian 11" /><published>2023-02-17T16:45:16+01:00</published><updated>2023-02-17T16:45:16+01:00</updated><id>/iaw/2023/02/17/jenkins-instalacion</id><content type="html" xml:base="/iaw/2023/02/17/jenkins-instalacion.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>Tenemos muchos métodos para realizar la instalación de Jenkins: desde un paquete, desde un contenedor, desde un fichero WAR, etc. En este caso vamos a realizar la instalación desde un contenedor, en concreto desde un contenedor de Docker.</p>

<h2 id="instalación-de-jenkins">Instalación de Jenkins</h2>

<ol>
  <li>
    <p>Vamos a crear un contenedor con debian, y vamos a usar el usuario root en la imagen.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> docker run <span class="nt">-d</span> <span class="nt">-p</span> 8080:8080 <span class="nt">-p</span> 50000:50000 <span class="nt">--name</span> jenkins <span class="nt">-v</span> jenkins_home:/var/jenkins_home jenkins/jenkins:lts-jdk11
</code></pre></div>    </div>

    <p><img src="/assets/images/jenkins/instalacion/1.png" alt="1" /></p>
  </li>
  <li>
    <p>Para obtener la la contraseña de administración que nos pregunta al principio ejecutamos:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> docker <span class="nb">exec </span>jenkins <span class="nb">cat</span> /var/jenkins_home/secrets/initialAdminPassword
 028d75e925ba4afea7664579be5915b4
</code></pre></div>    </div>

    <p><img src="/assets/images/jenkins/instalacion/2.png" alt="2" /></p>
  </li>
  <li>
    <p>Ingresamos en nuestro navegador la dirección http://localhost:8080 e introducimos la contraseña que hemos obtenido en el paso anterior y pulsamos en continuar.</p>

    <p><img src="/assets/images/jenkins/instalacion/3.png" alt="3" /></p>
  </li>
  <li>
    <p>Seleccionamos la opción de instalar los plugins sugeridos y pulsamos en continuar.</p>

    <p><img src="/assets/images/jenkins/instalacion/4.png" alt="4" /></p>
  </li>
  <li>
    <p>Configuramos el usuario administrador y pulsamos en continuar.</p>

    <p><img src="/assets/images/jenkins/instalacion/5.png" alt="5" /></p>
  </li>
  <li>
    <p>Seleccionamos la dirección de Jenkins y pulsamos en continuar.</p>

    <p><img src="/assets/images/jenkins/instalacion/6.png" alt="6" /></p>
  </li>
  <li>
    <p>Y todo estaría listo y funcionando</p>

    <p><img src="/assets/images/jenkins/instalacion/7.png" alt="7" /></p>

    <p><img src="/assets/images/jenkins/instalacion/8.png" alt="8" /></p>
  </li>
</ol>]]></content><author><name></name></author><category term="IAW" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Escenario - Poblar un directorio LDAP desde un fichero CSV</title><link href="/aso/2023/02/12/ldap2.html" rel="alternate" type="text/html" title="Escenario - Poblar un directorio LDAP desde un fichero CSV" /><published>2023-02-12T16:45:16+01:00</published><updated>2023-02-12T16:45:16+01:00</updated><id>/aso/2023/02/12/ldap2</id><content type="html" xml:base="/aso/2023/02/12/ldap2.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En nuestro <a href="https://sysmaria.netlify.app/hlc+sri/2022/12/05/escenario.html">escenario</a> vamos a realizar la instalación y configuración de OpenLDAP en la máquina <code class="language-plaintext highlighter-rouge">Alfa</code>. Lo haremos utilizando como base el nombre DNS asignado a la máquina, <code class="language-plaintext highlighter-rouge">alfa.mariajesus.gonzalonazareno.org</code>.</p>

<p>En esta ocasión, vamos a crear entre todos los alumnos de la clase, los que vayamos a hacer dicha práctica, un fichero CSV que incluya la siguiente información:</p>

<ul>
  <li>Nombre del alumno</li>
  <li>Apellidos del alumno</li>
  <li>Dirección de correo electrónico</li>
  <li>Nombre del equipo</li>
  <li>Dirección IP del equipo</li>
  <li>Clave pública SSH del equipo</li>
</ul>

<p>El mío quedaría así:</p>

<pre><code class="language-csv">Maria Jesus,Alloza,Rodriguez,mariajesus.allozarodriguez@gmail.com,maria,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDofL1X9E8awIaBBO3RO4fsgi1MgqRSSbx8eJIVjdEJMgIA0ZMlYGppxUADGCdKTZdTLOZwPkwnD53ydWKU+zmu806hGHpdKQD8UkmCvRTO1tNcDdWYeGU9kNbsuxezzdMBpumjx6XQ/IobvXSfG6NVFQmU1GbtVtAHPNKRSO5ql0irL/dUOK/aoNvat0HD182ShRUWCrkzio7iNXD2yFuNyu3KCcdi06bcUQOXj3nfTlzBRPcEawXxOCOaKMm5Za/huGfM9DNrS58vB+j+0Q5qLDPMWV72c9tXHhHw+/ESU8EKh3v7jpBdDt7i6PpEy9Y4p1RUEOfBkJ9pcstLsz6od5AFcs0fqptBuDR1kPwMlT4wSSqSo7K7F19PlywjUTPNEPlzjc2/meNDGBJNMQXjx5FJjivtkPueL5fR/yN0ubGgOQmDRZ2tkl8LZ5UPXSHbxJ6e4RjPYXzMX9Nc7vFbdkZbMudRy40fvxqL+OSnvZ1Y/3OQqf7NQdoNJdfLHrM= maria@delta.mariajesus.gonzalonazareno.org
Ivan,Pina Castillo,ivanpicas88@gmail.com,ivan,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCfYm123AV12rRYM+tPkrd0Hrzc9Py32Ov8JVZCnH5zVBj3I/IxE08LUhccSSx9aD0DrW+RdfpmLCSBTgGnbdM9eYlq3jxoBqqye4DQeXLSPyXcp/qRPGPsNO+eGypVhRB+Oq9B+ktrHgzAXQSP1yjmjN57H7GVBnMEJhpCEVXk5vWgMhVNxsDSF6lHrbiaYLtunTtt+fNgrprzXuUqhUwEDRt6/ktwad420J7kmqkB4dQuex3hV+16l1GyNH8AJzNzoinTiLr/jW8Ja0udgIknsxFvZ5Df+ACCrXfIFwvdPTm6Nya0jCm9vFx5yc5O1E07qlbAAn3FiIfS5Udjs6rNZjfFH5GmlpodhcGy4nkCYZvylnEayIa/ak4wA7oDft60hlHBMCHMoyY3ZcIkWGmVkwnTB3xfxfykPeD14zQAlIuMol9RNmPUbDYtbfY64npLPmagUIHSpwwbs1byEBbzzqzG8qcCAPUk3mK6oB5+OKUNJDv+4MM+suj+Y/PnWM8= debian@delta
Juan Jesus,Alejo Sillero,juanjesusalejosillero@gmail.com,juanje,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDzLgcT2qKvflOcKWjUGX0ecoVWN+phHx7dEws3b/rY/xAichGJ6oP8ucD4lVJ8XVrEOaUgqQ1laPK33+u9MmzARx4g17/jKcwepUWdkKVA2++RWG3bsNgxCCkR1Gi7XMAAjwq8/17OjCj+4bvfTPlW5FSjDqaLhfqqeDtKpFJ3wjGG5sjNPC0GU4cRKzggZaR40ld7siaOiMteQ8X6bIggeXw+ULGiUhB4/uoLu0z69AzGgDfoPJuJEx4pPlcnOip/TAuL/pUjTjdUUDTsrZSJegWoLmRwylKvwtX8WojqI2TnTOyLT0IG1oStq4gC4AKOiCqfiBOm25bFfX0lW0uUaR1RjEuGz3jV0vkH3pCiuarNk5KnEQQqUO0x6ZvdCvOlsWYoiDQ6MclGKfUkUzC1uST5khs4xB1zQAZ5795on4SV8STASTwjpxuTuk7v4lTxrm8bTAF4bWiezgOQ0aFr7P0APygX0rbCR1aXoGfSyrrvqOUtkUzpiWwZwOpj0K0= juanjesus@delta.juanjesus.gonzalonazareno.org
Alejandro,Montes Delgado,aaleemd11@gmail.com,alemd,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCvd5Lg3K1k1WeHYHE3T5aPSEx/duCM87J2LFhgFODU3Z0+1mE5bjDUzvrUTVoHdMu1phOd8IS2vPCL2Ei8PjooLdLrsqHtFN9JYvtBRRhTs5tV3JSGaaFrljYtiITbulVTHBTqssrr4VfJ1jxMF9aSVcPjfVxsNMo01AhK8PAnCiSVBru9BUYgoG+7rCVMeB78qTXyuKNLoFqsuIPmQeV8D0JGKGozS2cikflyEOgYC+UwILUliV0dVtlpPltOOMLKngWF1fyjnfsJykiULI+l20Iid5QtDx5SK6nLRTtrr9MYCZbSJqAKVsxxKIYY5I+qSNsikgNrTnWJVq6z4zr5u4AqazjiDqX+266cDBf7rEq9GA9OZuul4/Kzy+PB9+w5rCgneSP7iL38ehQSjWzfpD1ny8mxCFmwuEwFBpGYhYHNywBYg8f1iMayjBeatFIU8WUCrCct0y1Zlf/Hn+Lzd5xb4aJgWd+mMyMmhZPcTAC6UN03tM2b0s6wjP8DzJc= ubuntu@delta
</code></pre>

<h3 id="esquema-openssh-lpk">Esquema openssh-lpk</h3>

<p>Tras haber creado los ficheros, vamos a añadir el esquema openssh-lpk al servidor LDAP. Para ello, vamos a crear un fichero llamado openssh-lpk.ldif:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi /etc/ldap/schema/openssh-lpk.ldif
</code></pre></div></div>

<p>Y añadimos el siguiente contenido:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dn: <span class="nv">cn</span><span class="o">=</span>openssh-lpk,cn<span class="o">=</span>schema,cn<span class="o">=</span>config
objectClass: olcSchemaConfig
cn: openssh-lpk
olcAttributeTypes: <span class="o">(</span> 1.3.6.1.4.1.24552.500.1.1.1.13 NAME <span class="s1">'sshPublicKey'</span>
  DESC <span class="s1">'MANDATORY: OpenSSH Public key'</span>
  EQUALITY octetStringMatch
  SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 <span class="o">)</span>
olcObjectClasses: <span class="o">(</span> 1.3.6.1.4.1.24552.500.1.1.2.0 NAME <span class="s1">'ldapPublicKey'</span> SUP top AUXILIARY
  DESC <span class="s1">'MANDATORY: OpenSSH LPK objectclass'</span>
  MAY <span class="o">(</span> sshPublicKey <span class="nv">$ </span>uid <span class="o">)</span>
  <span class="o">)</span>
</code></pre></div></div>

<p>Lo siguiente será añadir el esquema al directorio que utilizamos para el servidor LDAP ejecutando el siguiente comando en nuestra terminal en la máquina <code class="language-plaintext highlighter-rouge">alfa</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ldapadd <span class="nt">-Y</span> EXTERNAL <span class="nt">-H</span> ldapi:/// <span class="nt">-f</span> /etc/ldap/schema/openssh-lpk.ldif
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap2/1.png" alt="1" /></p>

<h3 id="script-en-python-">Script en Python 🐍</h3>

<p>Para realizar el script, primero deberemos crear un entorno virtual. Para ello, ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv ldap
</code></pre></div></div>

<p>El fin del entorno virtual es instalar el módulo <code class="language-plaintext highlighter-rouge">python-ldap</code> que nos permitirá conectarnos al servidor LDAP. Para ello, activamos el entorno virtual y ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>ldap/bin/activate
pip <span class="nb">install </span>python3-ldap
pip <span class="nb">install </span><span class="nv">ldap3</span><span class="o">==</span>2.6
</code></pre></div></div>

<p>Una vez instalado el módulo, vamos a crear el script. Para ello, creamos un fichero llamado <code class="language-plaintext highlighter-rouge">ldap.py</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi ldap.py
</code></pre></div></div>

<p>Y añadimos el siguiente contenido:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env python
</span>
<span class="kn">import</span> <span class="nn">ldap3</span>
<span class="kn">from</span> <span class="nn">ldap3</span> <span class="kn">import</span> <span class="n">Connection</span><span class="p">,</span> <span class="n">ALL</span>
<span class="kn">from</span> <span class="nn">getpass</span> <span class="kn">import</span> <span class="n">getpass</span>
<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="nb">exit</span>

<span class="c1">### VARIABLES
</span>
<span class="c1"># Shell que se le asigna a los usuarios
</span><span class="n">shell</span> <span class="o">=</span> <span class="s">'/bin/bash'</span>

<span class="c1"># Ruta absoluta del directorio que contiene los directorios personales de los usuarios. Terminado en "/"
</span><span class="n">home_dir</span> <span class="o">=</span> <span class="s">'/home/ldap'</span>

<span class="c1"># El valor inicial para los UID que se asignan al insertar usuarios. 
</span><span class="n">uid_number</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="c1"># El GID que se le asigna a los usuarios. Si no se manda al anadir el usuario da error.
</span><span class="n">gid</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="c1">### VARIABLES
</span>
<span class="c1"># Leemos el fichero .csv de los usuarios y guardamos cada linea en una lista.
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'usuarios.csv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">usuarios</span><span class="p">:</span>
  <span class="n">usuarios</span> <span class="o">=</span> <span class="n">usuarios</span><span class="p">.</span><span class="n">readlines</span><span class="p">()</span>


<span class="c1">### Parametros para la conexion
</span><span class="n">ldap_ip</span> <span class="o">=</span> <span class="s">'ldap://alfa.mariajesus.gonzalonazareno.org:636'</span>
<span class="n">dominio_base</span> <span class="o">=</span> <span class="s">'dc=mariajesus,dc=gonzalonazareno,dc=org'</span>
<span class="n">user_admin</span> <span class="o">=</span> <span class="s">'admin'</span> 
<span class="n">contrasena</span> <span class="o">=</span> <span class="n">getpass</span><span class="p">(</span><span class="s">'Contrasena: '</span><span class="p">)</span>

<span class="c1"># Intenta realizar la conexion.
</span><span class="n">conn</span> <span class="o">=</span> <span class="n">Connection</span><span class="p">(</span><span class="n">ldap_ip</span><span class="p">,</span> <span class="s">'cn={},{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">user_admin</span><span class="p">,</span> <span class="n">dominio_base</span><span class="p">),</span><span class="n">contrasena</span><span class="p">)</span>

<span class="c1"># conn.bind() devuelve "True" si se ha establecido la conexion y "False" en caso contrario.
</span>
<span class="c1"># Si no se establece la conexion imprime por pantalla un error de conexion.
</span><span class="k">if</span> <span class="ow">not</span> <span class="n">conn</span><span class="p">.</span><span class="n">bind</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'No se ha podido conectar con ldap'</span><span class="p">)</span> 
  <span class="k">if</span> <span class="n">conn</span><span class="p">.</span><span class="n">result</span><span class="p">[</span><span class="s">'description'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'invalidCredentials'</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Credenciales no validas.'</span><span class="p">)</span>
  <span class="c1"># Termina el script.
</span>  <span class="nb">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Recorre la lista de usuarios
</span><span class="k">for</span> <span class="n">user</span> <span class="ow">in</span> <span class="n">usuarios</span><span class="p">:</span>
  <span class="c1"># Separa los valores del usuario usando como delimitador ",", y asigna cada valor a la variable correspondiente.
</span>  <span class="n">user</span> <span class="o">=</span> <span class="n">user</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">','</span><span class="p">)</span>
  <span class="n">cn</span> <span class="o">=</span> <span class="n">user</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">sn</span> <span class="o">=</span> <span class="n">user</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">mail</span> <span class="o">=</span> <span class="n">user</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
  <span class="n">uid</span> <span class="o">=</span> <span class="n">user</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
  <span class="n">ssh</span> <span class="o">=</span> <span class="n">user</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

  <span class="c1">#Anade el usuario.
</span>  <span class="n">conn</span><span class="p">.</span><span class="n">add</span><span class="p">(</span>
    <span class="s">'uid={},ou=Personas,{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">uid</span><span class="p">,</span> <span class="n">dominio_base</span><span class="p">),</span>
    <span class="n">object_class</span> <span class="o">=</span> 
      <span class="p">[</span>
      <span class="s">'inetOrgPerson'</span><span class="p">,</span>
      <span class="s">'posixAccount'</span><span class="p">,</span> 
      <span class="s">'ldapPublicKey'</span>
      <span class="p">],</span>
    <span class="n">attributes</span> <span class="o">=</span>
      <span class="p">{</span>
      <span class="s">'cn'</span><span class="p">:</span> <span class="n">cn</span><span class="p">,</span>
      <span class="s">'sn'</span><span class="p">:</span> <span class="n">sn</span><span class="p">,</span>
      <span class="s">'mail'</span><span class="p">:</span> <span class="n">mail</span><span class="p">,</span>
      <span class="s">'uid'</span><span class="p">:</span> <span class="n">uid</span><span class="p">,</span>
      <span class="s">'uidNumber'</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">uid_number</span><span class="p">),</span>
      <span class="s">'gidNumber'</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">gid</span><span class="p">),</span>
      <span class="s">'homeDirectory'</span><span class="p">:</span> <span class="s">'{}{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">home_dir</span><span class="p">,</span><span class="n">uid</span><span class="p">),</span>
      <span class="s">'loginShell'</span><span class="p">:</span> <span class="n">shell</span><span class="p">,</span>
      <span class="s">'sshPublicKey'</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">ssh</span><span class="p">)</span>
      <span class="p">})</span>

  <span class="k">if</span> <span class="n">conn</span><span class="p">.</span><span class="n">result</span><span class="p">[</span><span class="s">'description'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'entryAlreadyExists'</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'El usuario {} ya existe.'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">uid</span><span class="p">))</span>

  <span class="c1"># Aumenta el contador para asignar un UID diferente a cada usuario (cada vez que ejecutemos el script debemos asegurarnos de ante mano que no existe dicho uid en el directorio ldap, o se solaparian los datos)
</span>  <span class="n">uid_number</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1">#Cierra la conexion.
</span><span class="n">conn</span><span class="p">.</span><span class="n">unbind</span><span class="p">()</span>
</code></pre></div></div>

<p>Lo siguiente serña comprobar que el script funciona correctamente. Para ello, ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 ldap.py
</code></pre></div></div>

<p>Comprobamos que se han añadido los usuarios correctamente en el directorio LDAP:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ldapsearch <span class="nt">-x</span> <span class="nt">-LLL</span> <span class="nt">-b</span> <span class="nv">dc</span><span class="o">=</span>mariajesus,dc<span class="o">=</span>gonzalonazareno,dc<span class="o">=</span>org <span class="nt">-D</span> <span class="s2">"cn=admin,dc=mariajesus,dc=gonzalonazareno,dc=org"</span> <span class="nt">-W</span>
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap2/2.png" alt="2" /></p>

<h3 id="configuramos-el-sistema-para-que-los-usuarios-sean-autenticados-por-ldap">Configuramos el sistema para que los usuarios sean autenticados por LDAP</h3>

<p>Para que los usuarios sean autenticados por LDAP, debemos modificar el archivo <code class="language-plaintext highlighter-rouge">/etc/ldap/ldap.conf</code> y añadir las siguientes líneas:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BASE <span class="nv">dc</span><span class="o">=</span>mariajesus,dc<span class="o">=</span>gonzalonazareno,dc<span class="o">=</span>org
URI ldap://alfa.mariajesus.gonzalonazareno.org
</code></pre></div></div>

<p>También deberemos modificar el archivo <code class="language-plaintext highlighter-rouge">/etc/nsswitch.conf</code> y añadir la siguiente línea:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>passwd:         files systemd ldap
group:          files systemd ldap
shadow:         files ldap
</code></pre></div></div>

<p>Instalamos el paquete <code class="language-plaintext highlighter-rouge">libpam-ldap</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt <span class="nb">install </span>libpam-ldap
</code></pre></div></div>

<p>Y lo configuramos:</p>

<ul>
  <li>LDAP server URI: `ldap://127.0.0.1</li>
  <li>Distinguished name of search base: <code class="language-plaintext highlighter-rouge">dc=mariajesus,dc=gonzalonazareno,dc=org</code></li>
  <li>LDAP version: <code class="language-plaintext highlighter-rouge">3</code></li>
  <li>LDAP acount for root DN: <code class="language-plaintext highlighter-rouge">cn=admin,dc=mariajesus,dc=gonzalonazareno,dc=org</code></li>
</ul>

<h3 id="configuramos-el-sistema-para-que-los-usuarios-puedan-acceder-por-ssh">Configuramos el sistema para que los usuarios puedan acceder por SSH</h3>

<p>Ante todo, vamos aconfigurarlo para que dichos usuarios tengan su propia <code class="language-plaintext highlighter-rouge">/home</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"session   requiered    pam_mkhomedir.so"</span> <span class="o">&gt;&gt;</span> /etc/pam.d/common-session
</code></pre></div></div>

<p>Ahora, le toca el turno a la configuración del sistema, para que los usuarios puedan acceder por SSH. Para ello, crearemos un script que se encargará de buscar las claves. Lo añadiremos aldirectorio <code class="language-plaintext highlighter-rouge">/opt</code> para que tenga los permisos necesarios, estando en un dorectorio perteneciente a <code class="language-plaintext highlighter-rouge">root</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi /opt/search.sh

<span class="c">#!/bin/bash</span>

<span class="c"># Busca la clave publica del usuario en el directorio LDAP.</span>

ldapsearch <span class="nt">-x</span> <span class="nt">-u</span> <span class="nt">-LLL</span> <span class="nt">-o</span> ldif-wrap<span class="o">=</span>no <span class="s1">'(&amp;(objectClass=posixAccount)(uid='</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span><span class="s1">'))'</span> <span class="s1">'sshPublicKey'</span> | <span class="nb">sed</span> <span class="nt">-n</span> <span class="s1">'s/^[ \t]*sshPublicKey::[ \t]*\(.*\)/\1/p'</span> | <span class="nb">base64</span> <span class="nt">-d</span>
</code></pre></div></div>

<p>Este comando es bastante complejo, así que vamos a explicarlo:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ldapsearch</code>: comando para buscar en el directorio LDAP.</li>
  <li><code class="language-plaintext highlighter-rouge">-x</code>: autenticación simple.</li>
  <li><code class="language-plaintext highlighter-rouge">-u</code>: no mostrar el prompt.</li>
  <li><code class="language-plaintext highlighter-rouge">-LLL</code>: no mostrar los mensajes de error.</li>
  <li><code class="language-plaintext highlighter-rouge">-o ldif-wrap=no</code>: no mostrar los mensajes de error.</li>
  <li><code class="language-plaintext highlighter-rouge">(&amp;(objectClass=posixAccount)(uid='"$1"'))'</code>: busca el usuario que se le pasa como parámetro.</li>
  <li><code class="language-plaintext highlighter-rouge">sshPublicKey'</code>: busca la clave pública del usuario.</li>
  <li><code class="language-plaintext highlighter-rouge">sed -n 's/^[ \t]*sshPublicKey::[ \t]*\(.*\)/\1/p'</code>: elimina la parte de la clave pública que no nos interesa.</li>
  <li><code class="language-plaintext highlighter-rouge">base64 -d</code>: decodifica la clave pública.</li>
</ul>

<p>Una vez creado, le damos permisos de ejecución:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>755 /opt/search.sh
</code></pre></div></div>

<p>Para hacer una prueba, ejecutamos el script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> /opt/search.sh maria
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap2/3.png" alt="3" /></p>

<p>Tras esta comprobación, nos dirigimos al fichero <code class="language-plaintext highlighter-rouge">/etc/ssh/sshd_config</code> y añadimos las siguientes líneas:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AuthorizedKeysCommand /opt/search.sh
AuthorizedKeysCommandUser nobody
</code></pre></div></div>

<p>Reiniciamos el servicio SSH:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl restart sshd
</code></pre></div></div>

<p>Y comprabamos que funciona correctamente:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh maria@alfa
</code></pre></div></div>

<p><img src="/assets/images/LDAP/ldap2/4.png" alt="4" /></p>

<p>Para comprobar el funcionamiento completo, he añadido una nueva clave, la de mi compañero Antonio. He comprobado que se ha añadido correctamente en el directorio LDAP, pero al realizar la conexion, no le esposible dado que le pide una contraseña.</p>

<p><img src="/assets/images/LDAP/ldap2/5.png" alt="5" /></p>

<p><img src="/assets/images/LDAP/ldap2/6.png" alt="6" /></p>]]></content><author><name></name></author><category term="ASO" /><summary type="html"><![CDATA[Introducción En nuestro escenario vamos a realizar la instalación y configuración de OpenLDAP en la máquina Alfa. Lo haremos utilizando como base el nombre DNS asignado a la máquina, alfa.mariajesus.gonzalonazareno.org. En esta ocasión, vamos a crear entre todos los alumnos de la clase, los que vayamos a hacer dicha práctica, un fichero CSV que incluya la siguiente información: Nombre del alumno Apellidos del alumno Dirección de correo electrónico Nombre del equipo Dirección IP del equipo Clave pública SSH del equipo El mío quedaría así: Maria Jesus,Alloza,Rodriguez,mariajesus.allozarodriguez@gmail.com,maria,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDofL1X9E8awIaBBO3RO4fsgi1MgqRSSbx8eJIVjdEJMgIA0ZMlYGppxUADGCdKTZdTLOZwPkwnD53ydWKU+zmu806hGHpdKQD8UkmCvRTO1tNcDdWYeGU9kNbsuxezzdMBpumjx6XQ/IobvXSfG6NVFQmU1GbtVtAHPNKRSO5ql0irL/dUOK/aoNvat0HD182ShRUWCrkzio7iNXD2yFuNyu3KCcdi06bcUQOXj3nfTlzBRPcEawXxOCOaKMm5Za/huGfM9DNrS58vB+j+0Q5qLDPMWV72c9tXHhHw+/ESU8EKh3v7jpBdDt7i6PpEy9Y4p1RUEOfBkJ9pcstLsz6od5AFcs0fqptBuDR1kPwMlT4wSSqSo7K7F19PlywjUTPNEPlzjc2/meNDGBJNMQXjx5FJjivtkPueL5fR/yN0ubGgOQmDRZ2tkl8LZ5UPXSHbxJ6e4RjPYXzMX9Nc7vFbdkZbMudRy40fvxqL+OSnvZ1Y/3OQqf7NQdoNJdfLHrM= maria@delta.mariajesus.gonzalonazareno.org Ivan,Pina Castillo,ivanpicas88@gmail.com,ivan,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCfYm123AV12rRYM+tPkrd0Hrzc9Py32Ov8JVZCnH5zVBj3I/IxE08LUhccSSx9aD0DrW+RdfpmLCSBTgGnbdM9eYlq3jxoBqqye4DQeXLSPyXcp/qRPGPsNO+eGypVhRB+Oq9B+ktrHgzAXQSP1yjmjN57H7GVBnMEJhpCEVXk5vWgMhVNxsDSF6lHrbiaYLtunTtt+fNgrprzXuUqhUwEDRt6/ktwad420J7kmqkB4dQuex3hV+16l1GyNH8AJzNzoinTiLr/jW8Ja0udgIknsxFvZ5Df+ACCrXfIFwvdPTm6Nya0jCm9vFx5yc5O1E07qlbAAn3FiIfS5Udjs6rNZjfFH5GmlpodhcGy4nkCYZvylnEayIa/ak4wA7oDft60hlHBMCHMoyY3ZcIkWGmVkwnTB3xfxfykPeD14zQAlIuMol9RNmPUbDYtbfY64npLPmagUIHSpwwbs1byEBbzzqzG8qcCAPUk3mK6oB5+OKUNJDv+4MM+suj+Y/PnWM8= debian@delta Juan Jesus,Alejo Sillero,juanjesusalejosillero@gmail.com,juanje,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDzLgcT2qKvflOcKWjUGX0ecoVWN+phHx7dEws3b/rY/xAichGJ6oP8ucD4lVJ8XVrEOaUgqQ1laPK33+u9MmzARx4g17/jKcwepUWdkKVA2++RWG3bsNgxCCkR1Gi7XMAAjwq8/17OjCj+4bvfTPlW5FSjDqaLhfqqeDtKpFJ3wjGG5sjNPC0GU4cRKzggZaR40ld7siaOiMteQ8X6bIggeXw+ULGiUhB4/uoLu0z69AzGgDfoPJuJEx4pPlcnOip/TAuL/pUjTjdUUDTsrZSJegWoLmRwylKvwtX8WojqI2TnTOyLT0IG1oStq4gC4AKOiCqfiBOm25bFfX0lW0uUaR1RjEuGz3jV0vkH3pCiuarNk5KnEQQqUO0x6ZvdCvOlsWYoiDQ6MclGKfUkUzC1uST5khs4xB1zQAZ5795on4SV8STASTwjpxuTuk7v4lTxrm8bTAF4bWiezgOQ0aFr7P0APygX0rbCR1aXoGfSyrrvqOUtkUzpiWwZwOpj0K0= juanjesus@delta.juanjesus.gonzalonazareno.org Alejandro,Montes Delgado,aaleemd11@gmail.com,alemd,ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCvd5Lg3K1k1WeHYHE3T5aPSEx/duCM87J2LFhgFODU3Z0+1mE5bjDUzvrUTVoHdMu1phOd8IS2vPCL2Ei8PjooLdLrsqHtFN9JYvtBRRhTs5tV3JSGaaFrljYtiITbulVTHBTqssrr4VfJ1jxMF9aSVcPjfVxsNMo01AhK8PAnCiSVBru9BUYgoG+7rCVMeB78qTXyuKNLoFqsuIPmQeV8D0JGKGozS2cikflyEOgYC+UwILUliV0dVtlpPltOOMLKngWF1fyjnfsJykiULI+l20Iid5QtDx5SK6nLRTtrr9MYCZbSJqAKVsxxKIYY5I+qSNsikgNrTnWJVq6z4zr5u4AqazjiDqX+266cDBf7rEq9GA9OZuul4/Kzy+PB9+w5rCgneSP7iL38ehQSjWzfpD1ny8mxCFmwuEwFBpGYhYHNywBYg8f1iMayjBeatFIU8WUCrCct0y1Zlf/Hn+Lzd5xb4aJgWd+mMyMmhZPcTAC6UN03tM2b0s6wjP8DzJc= ubuntu@delta Esquema openssh-lpk Tras haber creado los ficheros, vamos a añadir el esquema openssh-lpk al servidor LDAP. Para ello, vamos a crear un fichero llamado openssh-lpk.ldif: vi /etc/ldap/schema/openssh-lpk.ldif Y añadimos el siguiente contenido: dn: cn=openssh-lpk,cn=schema,cn=config objectClass: olcSchemaConfig cn: openssh-lpk olcAttributeTypes: ( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' DESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) olcObjectClasses: ( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' SUP top AUXILIARY DESC 'MANDATORY: OpenSSH LPK objectclass' MAY ( sshPublicKey $ uid ) ) Lo siguiente será añadir el esquema al directorio que utilizamos para el servidor LDAP ejecutando el siguiente comando en nuestra terminal en la máquina alfa: ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/openssh-lpk.ldif Script en Python 🐍 Para realizar el script, primero deberemos crear un entorno virtual. Para ello, ejecutamos el siguiente comando: python3 -m venv ldap El fin del entorno virtual es instalar el módulo python-ldap que nos permitirá conectarnos al servidor LDAP. Para ello, activamos el entorno virtual y ejecutamos el siguiente comando: source ldap/bin/activate pip install python3-ldap pip install ldap3==2.6 Una vez instalado el módulo, vamos a crear el script. Para ello, creamos un fichero llamado ldap.py: vi ldap.py Y añadimos el siguiente contenido: #!/usr/bin/env python import ldap3 from ldap3 import Connection, ALL from getpass import getpass from sys import exit ### VARIABLES # Shell que se le asigna a los usuarios shell = '/bin/bash' # Ruta absoluta del directorio que contiene los directorios personales de los usuarios. Terminado en "/" home_dir = '/home/ldap' # El valor inicial para los UID que se asignan al insertar usuarios. uid_number = 5000 # El GID que se le asigna a los usuarios. Si no se manda al anadir el usuario da error. gid = 5000 ### VARIABLES # Leemos el fichero .csv de los usuarios y guardamos cada linea en una lista. with open('usuarios.csv', 'r') as usuarios: usuarios = usuarios.readlines() ### Parametros para la conexion ldap_ip = 'ldap://alfa.mariajesus.gonzalonazareno.org:636' dominio_base = 'dc=mariajesus,dc=gonzalonazareno,dc=org' user_admin = 'admin' contrasena = getpass('Contrasena: ') # Intenta realizar la conexion. conn = Connection(ldap_ip, 'cn={},{}'.format(user_admin, dominio_base),contrasena) # conn.bind() devuelve "True" si se ha establecido la conexion y "False" en caso contrario. # Si no se establece la conexion imprime por pantalla un error de conexion. if not conn.bind(): print('No se ha podido conectar con ldap') if conn.result['description'] == 'invalidCredentials': print('Credenciales no validas.') # Termina el script. exit(0) # Recorre la lista de usuarios for user in usuarios: # Separa los valores del usuario usando como delimitador ",", y asigna cada valor a la variable correspondiente. user = user.split(',') cn = user[0] sn = user[1] mail = user[2] uid = user[3] ssh = user[4] #Anade el usuario. conn.add( 'uid={},ou=Personas,{}'.format(uid, dominio_base), object_class = [ 'inetOrgPerson', 'posixAccount', 'ldapPublicKey' ], attributes = { 'cn': cn, 'sn': sn, 'mail': mail, 'uid': uid, 'uidNumber': str(uid_number), 'gidNumber': str(gid), 'homeDirectory': '{}{}'.format(home_dir,uid), 'loginShell': shell, 'sshPublicKey': str(ssh) }) if conn.result['description'] == 'entryAlreadyExists': print('El usuario {} ya existe.'.format(uid)) # Aumenta el contador para asignar un UID diferente a cada usuario (cada vez que ejecutemos el script debemos asegurarnos de ante mano que no existe dicho uid en el directorio ldap, o se solaparian los datos) uid_number += 1 #Cierra la conexion. conn.unbind() Lo siguiente serña comprobar que el script funciona correctamente. Para ello, ejecutamos el siguiente comando: python3 ldap.py Comprobamos que se han añadido los usuarios correctamente en el directorio LDAP: ldapsearch -x -LLL -b dc=mariajesus,dc=gonzalonazareno,dc=org -D "cn=admin,dc=mariajesus,dc=gonzalonazareno,dc=org" -W Configuramos el sistema para que los usuarios sean autenticados por LDAP Para que los usuarios sean autenticados por LDAP, debemos modificar el archivo /etc/ldap/ldap.conf y añadir las siguientes líneas: BASE dc=mariajesus,dc=gonzalonazareno,dc=org URI ldap://alfa.mariajesus.gonzalonazareno.org También deberemos modificar el archivo /etc/nsswitch.conf y añadir la siguiente línea: passwd: files systemd ldap group: files systemd ldap shadow: files ldap Instalamos el paquete libpam-ldap: apt install libpam-ldap Y lo configuramos: LDAP server URI: `ldap://127.0.0.1 Distinguished name of search base: dc=mariajesus,dc=gonzalonazareno,dc=org LDAP version: 3 LDAP acount for root DN: cn=admin,dc=mariajesus,dc=gonzalonazareno,dc=org Configuramos el sistema para que los usuarios puedan acceder por SSH Ante todo, vamos aconfigurarlo para que dichos usuarios tengan su propia /home: echo "session requiered pam_mkhomedir.so" &gt;&gt; /etc/pam.d/common-session Ahora, le toca el turno a la configuración del sistema, para que los usuarios puedan acceder por SSH. Para ello, crearemos un script que se encargará de buscar las claves. Lo añadiremos aldirectorio /opt para que tenga los permisos necesarios, estando en un dorectorio perteneciente a root: vi /opt/search.sh #!/bin/bash # Busca la clave publica del usuario en el directorio LDAP. ldapsearch -x -u -LLL -o ldif-wrap=no '(&amp;(objectClass=posixAccount)(uid='"$1"'))' 'sshPublicKey' | sed -n 's/^[ \t]*sshPublicKey::[ \t]*\(.*\)/\1/p' | base64 -d Este comando es bastante complejo, así que vamos a explicarlo: ldapsearch: comando para buscar en el directorio LDAP. -x: autenticación simple. -u: no mostrar el prompt. -LLL: no mostrar los mensajes de error. -o ldif-wrap=no: no mostrar los mensajes de error. (&amp;(objectClass=posixAccount)(uid='"$1"'))': busca el usuario que se le pasa como parámetro. sshPublicKey': busca la clave pública del usuario. sed -n 's/^[ \t]*sshPublicKey::[ \t]*\(.*\)/\1/p': elimina la parte de la clave pública que no nos interesa. base64 -d: decodifica la clave pública. Una vez creado, le damos permisos de ejecución: chmod 755 /opt/search.sh Para hacer una prueba, ejecutamos el script: source /opt/search.sh maria Tras esta comprobación, nos dirigimos al fichero /etc/ssh/sshd_config y añadimos las siguientes líneas: AuthorizedKeysCommand /opt/search.sh AuthorizedKeysCommandUser nobody Reiniciamos el servicio SSH: systemctl restart sshd Y comprabamos que funciona correctamente: ssh maria@alfa Para comprobar el funcionamiento completo, he añadido una nueva clave, la de mi compañero Antonio. He comprobado que se ha añadido correctamente en el directorio LDAP, pero al realizar la conexion, no le esposible dado que le pide una contraseña.]]></summary></entry><entry><title type="html">Taller Kubernetes: Instalación de un CMS con Helm</title><link href="/hlc+sri/2023/02/12/kubernetes-cms.html" rel="alternate" type="text/html" title="Taller Kubernetes: Instalación de un CMS con Helm" /><published>2023-02-12T16:45:16+01:00</published><updated>2023-02-12T16:45:16+01:00</updated><id>/hlc+sri/2023/02/12/kubernetes-cms</id><content type="html" xml:base="/hlc+sri/2023/02/12/kubernetes-cms.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En este taller vamos a instalar un CMS (Content Management System) en un cluster de Kubernetes. Para ello, vamos a instalar la última versión de Helm y añadiremos el repositorio de bitnami, porque instalaremos un wordpress.</p>

<h2 id="instalamos-la-última-versión-de-helm">Instalamos la última versión de Helm</h2>

<ol>
  <li>
    <p>Instalamos la última versión de Helm. Como nos indican en su <a href="https://helm.sh/docs/intro/install/">web oficial</a>, vamos a descargarnos el binario de la última versión de Helm y lo vamos a instalar en nuestro sistema.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nv">$ </span>curl <span class="nt">-fsSL</span> <span class="nt">-o</span> get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
 <span class="nv">$ </span><span class="nb">chmod </span>700 get_helm.sh
 <span class="nv">$ </span>./get_helm.sh
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t7/1.png" alt="1" /></p>
  </li>
  <li>
    <p>Añadimos el repositorio de bitnami.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nv">$ </span>helm repo add bitnami https://charts.bitnami.com/bitnami
</code></pre></div>    </div>

    <p>Y comprobamos que se ha añadido correctamente.</p>

    <p><img src="/assets/images/kubernetes/t7/2.png" alt="2" /></p>
  </li>
  <li>
    <p>Acualizamos los repositorios.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nv">$ </span>helm repo update
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t7/3.png" alt="3" /></p>
  </li>
  <li>
    <p>Buscamos el chart de bitnami para wordpress y lo instalamos.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nv">$ </span>helm search repo wordpress
 <span class="nv">$ </span>helm <span class="nb">install </span>server bitnami/wordpress <span class="nt">--set</span> service.type<span class="o">=</span>NodePort <span class="nt">--set</span> <span class="nv">wordpressBlogName</span><span class="o">=</span><span class="s2">"mariatec"</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t7/4.png" alt="4" /></p>

    <p>Y realizamos los pasos que nos indica el comando para acceder a la aplicación.</p>

    <p><img src="/assets/images/kubernetes/t7/5.png" alt="5" /></p>
  </li>
</ol>

<p>Como podemos ver, hemos instalado un CMS en nuestro cluster de Kubernetes.</p>

<p><img src="/assets/images/kubernetes/t7/6.png" alt="6" /></p>

<p>Entramos en la url que nos ha dado el comando y vemos que tenemos un wordpress instalado.</p>

<p><img src="/assets/images/kubernetes/t7/7.png" alt="7" /></p>

<p>Y si entramos con las credenciales que nos ha dado el comando, podemos ver acceder a la zona de administración.</p>

<p><img src="/assets/images/kubernetes/t7/8.png" alt="8" /></p>]]></content><author><name></name></author><category term="HLC+SRI" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Despliegue de una aplicación en Kubernetes</title><link href="/hlc+sri/2023/02/12/kubernetes-p1.html" rel="alternate" type="text/html" title="Despliegue de una aplicación en Kubernetes" /><published>2023-02-12T16:45:16+01:00</published><updated>2023-02-12T16:45:16+01:00</updated><id>/hlc+sri/2023/02/12/kubernetes-p1</id><content type="html" xml:base="/hlc+sri/2023/02/12/kubernetes-p1.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En <a href="https://sysmaria.netlify.app/iaw/2023/02/09/docker-bookmedik.html">este post</a> realizamos la implantación de una aplicación en Docker. En este post vamos a realizar el despliegue de la misma aplicación en Kubernetes.</p>

<h2 id="ejercicio-1-despliegue-en-minikube">Ejercicio 1: Despliegue en minikube</h2>
<p>En mi casi caso, voy a elegir bookmedik como aplicación a desplegar. Para ello, voy a utilizar el siguiente <a href="">repositorio</a> que contiene el código de la aplicación.</p>

<h3 id="despliegue-de-la-aplicación">Despliegue de la aplicación</h3>

<ol>
  <li>
    <p>Creamos los ficheros yaml para crear un <code class="language-plaintext highlighter-rouge">ConfigMap</code> y un <code class="language-plaintext highlighter-rouge">Secret</code> donde guardaremos las variables de entorno.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl create cm cm-mariadb <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">mysql_usuario</span><span class="o">=</span>bookmedik <span class="se">\</span>
 <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">basededatos</span><span class="o">=</span>bookmedik <span class="se">\</span>
 <span class="nt">-o</span> yaml <span class="nt">--dry-run</span><span class="o">=</span>client <span class="o">&gt;</span> bd_datos_configmap.yaml

 kubectl create secret generic secret-mariadb <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">password</span><span class="o">=</span>bookmedik <span class="se">\</span>
 <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">rootpass</span><span class="o">=</span>root <span class="se">\</span>
 <span class="nt">-o</span> yaml <span class="nt">--dry-run</span><span class="o">=</span>client <span class="o">&gt;</span> bd_passwords_secret.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/1.png" alt="1" /></p>
  </li>
  <li>
    <p>Creamos el volumen y el fichero de despliegue para <code class="language-plaintext highlighter-rouge">mariadb</code></p>

    <ul>
      <li>
        <p>Volumen para mariadb</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
  <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mariadb-pvc</span>
  <span class="na">spec</span><span class="pi">:</span>
      <span class="na">accessModes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
      <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
          <span class="na">storage</span><span class="pi">:</span> <span class="s">3Gi</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Despliegue de mariadb</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mariadb</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">mariadb</span>
    <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">mariadb</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">mariadb</span>
        <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-mariadb</span>
          <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
            <span class="na">claimName</span><span class="pi">:</span> <span class="s">mariadb-pvc</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-mariadb</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">mariadb:10.5</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MARIADB_ROOT_PASSWORD</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">secretKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">secret-mariadb</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">rootpass</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MARIADB_DATABASE</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">configMapKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">cm-mariadb</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">basededatos</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MARIADB_USER</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">configMapKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">cm-mariadb</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">mysql_usuario</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MARIADB_PASSWORD</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">secretKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">secret-mariadb</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">password</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">mariadb-server</span>
              <span class="na">containerPort</span><span class="pi">:</span> <span class="m">3306</span>
          <span class="na">volumeMounts</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/lib/mysql"</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-mariadb</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Servicio de mariadb</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mariadb</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">mariadb</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">3306</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="s">mariadb-server</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">mariadb</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
    <p>Creamos los ficheros de despliegue y servicios para bookmedik.</p>

    <ul>
      <li>
        <p>Despliegue de bookmedik</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bookmedik</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">bookmedik</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">matchLabels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">bookmedik</span>
        <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
    <span class="na">template</span><span class="pi">:</span>
      <span class="na">metadata</span><span class="pi">:</span>
        <span class="na">labels</span><span class="pi">:</span>
          <span class="na">app</span><span class="pi">:</span> <span class="s">bookmedik</span>
          <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-bookmedik</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">legnakra/bookmedik:latest</span>
      <span class="na">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">USUARIO_BOOKMEDIK</span>
          <span class="na">valueFrom</span><span class="pi">:</span>
            <span class="na">configMapKeyRef</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cm-mariadb</span>
              <span class="na">key</span><span class="pi">:</span> <span class="s">mysql_user</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">PASS_BOOKMEDIK</span>
          <span class="na">valueFrom</span><span class="pi">:</span>
            <span class="na">secretKeyRef</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">secret-mariadb</span>
              <span class="na">key</span><span class="pi">:</span> <span class="s">mysql_password</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">BASE_DATOS_BOOKMEDIK</span>
          <span class="na">valueFrom</span><span class="pi">:</span>
            <span class="na">configMapKeyRef</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cm-mariadb</span>
              <span class="na">key</span><span class="pi">:</span> <span class="s">mysql_database</span>
      <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http-server</span>
          <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Servicio de bookmedik</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
  <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bookmedik</span>
    <span class="na">labels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">bookmedik</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http-server</span>
    <span class="na">selector</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">bookmedik</span>
      <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
    <p>Creamos todo y anotamos la primera versión de la aplicación.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> <span class="nb">.</span>
</code></pre></div>    </div>

    <p>Comprobamos que los recursos se han creado correctamente.</p>

    <p><img src="/assets/images/kubernetes/p1/2.png" alt="2" /></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl annotate deployment.apps/bookmedik kubernetes.io/change-cause<span class="o">=</span><span class="s2">"Versión 1"</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/3.png" alt="3" /></p>
  </li>
  <li>
    <p>Accedemos a la aplicación y comprobamos que funciona correctamente.</p>

    <p><img src="/assets/images/kubernetes/p1/4.png" alt="4" /></p>

    <p>Y comprobamos que podemos acceder:</p>

    <p><img src="/assets/images/kubernetes/p1/5.png" alt="5" /></p>
  </li>
  <li>
    <p>Creamos el fichero Ingress para acceder a la aplicación desde el exterior.</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">bookmedik-ingress</span>
   <span class="na">annotations</span><span class="pi">:</span>
     <span class="na">nginx.ingress.kubernetes.io/rewrite-target</span><span class="pi">:</span> <span class="s">/</span>
 <span class="s">spec:nano</span>
   <span class="s">rules</span><span class="err">:</span>
   <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">www.maria-bookmedik.org</span>
     <span class="na">http</span><span class="pi">:</span>
       <span class="na">paths</span><span class="pi">:</span>
       <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
         <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
         <span class="na">backend</span><span class="pi">:</span>
           <span class="na">service</span><span class="pi">:</span>
             <span class="na">name</span><span class="pi">:</span> <span class="s">bookmedik</span>
             <span class="na">port</span><span class="pi">:</span>
               <span class="na">number</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div>    </div>

    <p>Y lo creamos:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> bookmedik-ingress.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/6.png" alt="6" /></p>
  </li>
  <li>
    <p>Modificamos el fichero <code class="language-plaintext highlighter-rouge">/etc/hosts</code> para que apunte a la IP del nodo maestro.</p>

    <p><img src="/assets/images/kubernetes/p1/7.png" alt="7" /></p>
  </li>
  <li>
    <p>Accedemos a la aplicación desde el navegador.</p>

    <p><img src="/assets/images/kubernetes/p1/8.png" alt="8" /></p>

    <p>Y accedemos a la zona de administración:</p>

    <p><img src="/assets/images/kubernetes/p1/9.png" alt="9" /></p>
  </li>
  <li>
    <p>Creamos información para probar la persistencia de la base de datos cuando borremos la base de datos de mariadb.</p>

    <p><img src="/assets/images/kubernetes/p1/10.png" alt="10" /></p>

    <ul>
      <li>
        <p>Eliminamos el despliegue de mariadb.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete deployment.apps/mariadb
</code></pre></div>        </div>

        <p><img src="/assets/images/kubernetes/p1/11.png" alt="11" /></p>
      </li>
      <li>
        <p>Comprobamos que la base de datos no funciona.</p>

        <p><img src="/assets/images/kubernetes/p1/12.png" alt="12" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Ahora nos toca escalar el despliegue de la aplicación a 3 réplicas y lo podemos hacer de dos formas:</p>

    <ul>
      <li>
        <p>Modificando el fichero de despliegue y aplicando los cambios, haciéndolo persistente.</p>
      </li>
      <li>
        <p>Modificando el despliegue en caliente, es decir, sin hacer persistente el cambio, ejecutando <code class="language-plaintext highlighter-rouge">kubectl scale &lt;deploy-name&gt; --replicas=&lt;num_replicas&gt;</code>.</p>
      </li>
    </ul>

    <p>En mi caso, quiero hacerlo de forma no persistente, por lo que ejecuto el comando:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  kubectl scale deployment.apps/bookmedik <span class="nt">--replicas</span><span class="o">=</span>3
</code></pre></div>    </div>

    <p>Y comprobamos que se ha escalado correctamente.</p>

    <p><img src="/assets/images/kubernetes/p1/13.png" alt="13" /></p>
  </li>
  <li>
    <p>Vamos a crear una imagen docker, pero con alguna modficación, para probar el despliegue de una nueva versión de la aplicación.</p>
  </li>
</ol>

<ul>
  <li>
    <p>Modificamos el fichero <code class="language-plaintext highlighter-rouge">index.php</code> de la aplicación para que muestre la versión de la aplicación.</p>

    <p><img src="/assets/images/kubernetes/p1/14.png" alt="14" /></p>

    <p>En mi caso, le he cambiado el texto de la página principal y de la pestaña del navegador.</p>
  </li>
  <li>
    <p>Creamos la imagen docker con la nueva versión de la aplicación.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> legnakra/bookmedik:v1_1 <span class="nb">.</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/15.png" alt="15" /></p>
  </li>
  <li>
    <p>Subimos la imagen a docker hub.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker push legnakra/bookmedik:v1_1
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/16.png" alt="16" /></p>
  </li>
  <li>
    <p>Modificamos el fichero de despliegue para que use la nueva versión de la imagen.</p>

    <p><img src="/assets/images/kubernetes/p1/17.png" alt="17" /></p>
  </li>
  <li>
    <p>Aplicamos los cambios.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> bookmedik-deployment.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/18.png" alt="18" /></p>

    <p><img src="/assets/images/kubernetes/p1/18-1.png" alt="18.1" /></p>
  </li>
  <li>
    <p>Comprobamos que se ha desplegado la nueva versión de la aplicación.</p>

    <p><img src="/assets/images/kubernetes/p1/19.png" alt="19" /></p>

    <p>Y que podemos acceder a la zona de administración.</p>

    <p><img src="/assets/images/kubernetes/p1/20.png" alt="20" /></p>
  </li>
</ul>

<h2 id="ejercicio-2-despliegue-en-otra-distribución-de-kubernetes">Ejercicio 2: Despliegue en otra distribución de kubernetes</h2>

<p>En esta parte, vamos a instalar un cluster de kubernetes. Pero lo haremos en una distribución diferente a la que hemos usado hasta ahora. Pero debemos hacerlo en otra versión, y he decidido hacerlo en k3s, que es una versión ligera de kubernetes.</p>

<ol>
  <li>
    <p>Creamos 3 nodos, uno maestro y dos esclavos.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># -*- mode: ruby -*-</span>
 <span class="c"># vi: set ft=ruby :</span>

 Vagrant.configure<span class="o">(</span><span class="s2">"2"</span><span class="o">)</span> <span class="k">do</span> |config|
   config.vm.define :controller <span class="k">do</span> |controller|
     controller.vm.box <span class="o">=</span> <span class="s2">"debian/buster64"</span>
     controller.vm.hostname <span class="o">=</span> <span class="s2">"controller"</span>
     controller.vm.network :private_network, ip: <span class="s2">"192.168.0.10"</span>
     controller.nfs.verify_installed <span class="o">=</span> <span class="nb">false
     </span>controller.vm.synced_folder <span class="s1">'.'</span>, <span class="s1">'/vagrant'</span>, disabled: <span class="nb">true
     </span>controller.vm.provider <span class="s2">"libvirt"</span> <span class="k">do</span> |v|
       v.memory <span class="o">=</span> 3072
       v.cpus <span class="o">=</span> 2
     end
   end
   config.vm.define :worker1 <span class="k">do</span> |worker1|
     worker1.vm.box <span class="o">=</span> <span class="s2">"debian/buster64"</span>
     worker1.vm.hostname <span class="o">=</span> <span class="s2">"worker1"</span>
     worker1.vm.network :private_network, ip: <span class="s2">"192.168.0.20"</span>
     worker1.nfs.verify_installed <span class="o">=</span> <span class="nb">false
     </span>worker1.vm.synced_folder <span class="s1">'.'</span>, <span class="s1">'/vagrant'</span>, disabled: <span class="nb">true
     </span>worker1.vm.provider <span class="s2">"libvirt"</span> <span class="k">do</span> |v|
       v.memory <span class="o">=</span> 3072
       v.cpus <span class="o">=</span> 2
     end
   end
   config.vm.define :worker2 <span class="k">do</span> |worker2|
     worker2.vm.box <span class="o">=</span> <span class="s2">"debian/buster64"</span>
     worker2.vm.hostname <span class="o">=</span> <span class="s2">"worker2"</span>
     worker2.vm.network :private_network, ip: <span class="s2">"192.168.0.30"</span>
     worker2.nfs.verify_installed <span class="o">=</span> <span class="nb">false
     </span>worker2.vm.synced_folder <span class="s1">'.'</span>, <span class="s1">'/vagrant'</span>, disabled: <span class="nb">true
     </span>worker2.vm.provider <span class="s2">"libvirt"</span> <span class="k">do</span> |v|
       v.memory <span class="o">=</span> 3072
       v.cpus <span class="o">=</span> 2
     end
   end
 end
</code></pre></div>    </div>
  </li>
  <li>
    <p>Instalamos k3s en el nodo maestro.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> curl <span class="nt">-sfL</span> https://get.k3s.io | sh -
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/21.png" alt="21" /></p>

    <p>Y comprobamos que está listo.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get nodes
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/22.png" alt="22" /></p>
  </li>
  <li>
    <p>Identificamos el token del nodo maestro.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">sudo cat</span> /var/lib/rancher/k3s/server/node-token
 K100ce50339c5ff211169afea38d299cc60ea62c65b98d102b49df1e1c535b8da7f::server:611c1ed3c8c978a4048e541fac5e1061
</code></pre></div>    </div>
  </li>
  <li>
    <p>Ejecutamos lo siguiente:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> curl <span class="nt">-sfL</span> https://get.k3s.io | <span class="nv">K3S_URL</span><span class="o">=</span>https://192.168.121.241:6443 <span class="nv">K3S_TOKEN</span><span class="o">=</span>K100ce50339c5ff211169afea38d299cc60ea62c65b98d102b49df1e1c535b8da7f::server:611c1ed3c8c978a4048e541fac5e1061 sh -
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/23.png" alt="23" /></p>

    <p><img src="/assets/images/kubernetes/p1/24.png" alt="24" /></p>

    <p>El comando anterior nos instala k3s en el nodo esclavo y lo añade al cluster. En el parámetro URL, debemos poner la IP del nodo maestro y en el parámetro token, el token que hemos obtenido en el paso anterior.</p>
  </li>
  <li>
    <p>Comprobamos que los nodos están listos.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get nodes
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/25.png" alt="25" /></p>
  </li>
  <li>
    <p>Instalamos git en el nodo maestro ( <code class="language-plaintext highlighter-rouge">controller</code>).</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> apt <span class="nb">install </span>git
</code></pre></div>    </div>
  </li>
  <li>
    <p>Clonamos el repositorio de la aplicación.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> git clone https://github.com/Legnakra/Kubernetes-bookmedik.git
</code></pre></div>    </div>
  </li>
  <li>
    <p>Rehacemos los pasos que hicimos en el ejercicio anterior:</p>

    <ul>
      <li>
        <p>Creamos los ficheros yaml para crear un <code class="language-plaintext highlighter-rouge">ConfigMap</code> y un <code class="language-plaintext highlighter-rouge">Secret</code> donde guardaremos las variables de entorno.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create cm cm-mariadb <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">mysql_usuario</span><span class="o">=</span>bookmedik <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">basededatos</span><span class="o">=</span>bookmedik

kubectl create secret generic secret-mariadb <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">password</span><span class="o">=</span>bookmedik <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">rootpass</span><span class="o">=</span>root 
</code></pre></div>        </div>

        <p><img src="/assets/images/kubernetes/p1/26.png" alt="26" /></p>
      </li>
      <li>
        <p>Creamos los recursos:</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> pvc-bookmedik.yaml
kubectl apply <span class="nt">-f</span> mariadb-deployment.yaml
kubectl apply <span class="nt">-f</span> mariadb-service.yaml
kubectl apply <span class="nt">-f</span> bookmedik-deployment.yaml  
kubectl apply <span class="nt">-f</span> bookmedik-service.yaml
kubectl apply <span class="nt">-f</span> bookmedik-ingress.yaml  
</code></pre></div>        </div>

        <p><img src="/assets/images/kubernetes/p1/27.png" alt="27" /></p>
      </li>
      <li>
        <p>Comprobamos que todo está listo.</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get all
</code></pre></div>        </div>

        <p><img src="/assets/images/kubernetes/p1/28.png" alt="28" /></p>

        <p><img src="/assets/images/kubernetes/p1/29.png" alt="29" /></p>
      </li>
      <li>
        <p>Accedemos a la dirección que hemos establecido en el fichero ‘bookmedik-ingress.yaml’.</p>

        <p><img src="/assets/images/kubernetes/p1/30.png" alt="30" /></p>

        <p>Y que podemos acceder a la zona de administración.</p>

        <p><img src="/assets/images/kubernetes/p1/31.png" alt="31" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Escalamos el despliegue como hemos hecho en el ejercicio anterior, y lo realizaremos con 3 replicas. Para ello, primero, veamos de nuevo que contamos con 2 replicas y vamos a extenderlo a 1 más.</p>
  </li>
</ol>

<ul>
  <li>
    <p>Antes de escalar el despliegue, vamos a ver las replicas que tenemos.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-o</span> wide
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/32.png" alt="32" /></p>
  </li>
  <li>
    <p>Ahora, vamos a escalar el despliegue.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl scale deployment.apps/bookmedik <span class="nt">--replicas</span><span class="o">=</span>3
</code></pre></div>    </div>
  </li>
  <li>
    <p>Comprobamos que se ha escalado correctamente y que tenemos 3 replicas.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-o</span> wide
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/p1/33.png" alt="33" /></p>
  </li>
  <li>
    <p>Comprobamos que podemos acceder a la aplicación.</p>

    <p><img src="/assets/images/kubernetes/p1/34.png" alt="34" /></p>

    <p>Y que podemos acceder a la zona de administración.</p>

    <p><img src="/assets/images/kubernetes/p1/35.png" alt="35" /></p>
  </li>
</ul>]]></content><author><name></name></author><category term="HLC+SRI" /><summary type="html"><![CDATA[Introducción]]></summary></entry><entry><title type="html">Implantación de aplicaciones web Python en docker</title><link href="/iaw/2023/02/11/docker-python.html" rel="alternate" type="text/html" title="Implantación de aplicaciones web Python en docker" /><published>2023-02-11T16:45:16+01:00</published><updated>2023-02-11T16:45:16+01:00</updated><id>/iaw/2023/02/11/docker-python</id><content type="html" xml:base="/iaw/2023/02/11/docker-python.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En este post vamos a configurar contenedores de Docker para ejecutar aplicaciones web Python. Para ello, vamos a utilizar una aplicación web Python sencilla que se encuentra en el repositorio de <a href="https://github.com/josedom24/django_tutorial">GitHub</a>.</p>

<h2 id="entorno-de-desarrollo">Entorno de desarrollo</h2>

<p>Vamos a crear un entorno de desarrollo en el que realizaremos las pruebas necesarias para la puesta a punto antes de lanzar nuestra aplicación en producción.</p>

<p>También vamos a crear dos contenedores que estarán conectados a la misma red. Una vez comprobado que la aplicación funciona, pasaremos a crear el docker-compose para pasarlo a producción.</p>

<h3 id="creación-de-la-red">Creación de la red</h3>

<p>Para crear la red, ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network create net_django
</code></pre></div></div>

<h3 id="creación-del-contenedor-de-la-base-de-datos">Creación del contenedor de la base de datos</h3>

<p>Ya creada la red en la que van a ir conectados los contenedores, nos ponemos manos a la obra y creamos el contenedor de la base de datos. Para ello, ejecutamos el siguiente comando:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-d</span> <span class="nt">--name</span> mariadb <span class="nt">-v</span> vol_polls:/var/lib/mysql <span class="nt">--network</span> net_django <span class="nt">-e</span> <span class="nv">MARIADB_ROOT_PASSWORD</span><span class="o">=</span>admin <span class="nt">-e</span> <span class="nv">MARIADB_USER</span><span class="o">=</span>django <span class="nt">-e</span> <span class="nv">MARIADB_PASSWORD</span><span class="o">=</span>admin <span class="nt">-e</span> <span class="nv">MARIADB_DATABASE</span><span class="o">=</span>django mariadb
</code></pre></div></div>

<p>Tendremos que modificar el fichero <code class="language-plaintext highlighter-rouge">settings.py</code> de la aplicación web para que sea capaz de leer las variables de entorno:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>

<span class="n">BASE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'default'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'ENGINE'</span><span class="p">:</span> <span class="s">'django.db.backends.mysql'</span><span class="p">,</span>
        <span class="s">'NAME'</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"BASE_DATOS"</span><span class="p">),</span>
        <span class="s">'USER'</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'USUARIO'</span><span class="p">),</span>
        <span class="s">'PASSWORD'</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"CONTRA"</span><span class="p">),</span>
        <span class="s">'HOST'</span><span class="p">:</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'HOST'</span><span class="p">),</span>
        <span class="s">'PORT'</span><span class="p">:</span> <span class="s">'3306'</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ALLOWED_HOSTS</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"ALLOWED_HOSTS"</span><span class="p">)]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">STATIC_ROOT</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">BASE_DIR</span><span class="p">,</span> <span class="s">'static'</span><span class="p">)</span>
<span class="n">STATIC_URL</span> <span class="o">=</span> <span class="s">'/static/'</span>
<span class="n">CSRF_TRUSTED_ORIGINS</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://*.mariatec.es'</span><span class="p">,</span><span class="s">'http://*.127.0.0.1'</span><span class="p">,</span><span class="s">'https://*.mariatec.es'</span><span class="p">,</span><span class="s">'https://*.127.0.0.1'</span><span class="p">]</span>
</code></pre></div></div>

<p>La estructura del directorio de la aplicación web es la siguiente:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── practica2
│   ├── django_tutorial
│   │   ├── manage.py
│   │   ├── requirements.txt
│   ├── docker-compose.yml
│   ├── Dockerfile
│   ├── polls.sh
</code></pre></div></div>

<p>Creamos el fichero <code class="language-plaintext highlighter-rouge">Dockerfile</code> a partir de la imagen de Python:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">FROM python:3</span>
<span class="s">WORKDIR /usr/src/app</span>
<span class="s">MAINTAINER Maria Jesús Alloza Rodríguez 'mariajesus.allozarodriguez@gmail.com'</span>
<span class="s">RUN apt-get install git &amp;&amp; pip install --root-user-action=ignore --upgrade pip &amp;&amp; pip install --root-user-action=ignore django mysqlclient</span>
<span class="s">RUN git clone https://github.com/Legnakra/django_tutorial.git /usr/src/app &amp;&amp; mkdir static</span>
<span class="s">ADD ./polls.sh /usr/src/app/</span>
<span class="s">RUN chmod +x /usr/src/app/polls.sh</span>
<span class="s">ENV ALLOWED_HOSTS='*'</span>
<span class="s">ENV HOST=mariadb</span>
<span class="s">ENV USUARIO=django</span>
<span class="s">ENV CONTRA=django</span>
<span class="s">ENV BASE_DATOS=django</span>
<span class="s">ENV DJANGO_SUPERUSER_PASSWORD=admin</span>
<span class="s">ENV DJANGO_SUPERUSER_USERNAME=admin</span>
<span class="s">ENV DJANGO_SUPERUSER_EMAIL=admin@example.org</span>
<span class="s">ENTRYPOINT ["/usr/src/app/polls.sh"]</span>
</code></pre></div></div>

<p>Una vez terminado de crear el fichero <code class="language-plaintext highlighter-rouge">Dockerfile</code>, creamos el fichero <code class="language-plaintext highlighter-rouge">django_polls.sh</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#! /bin/sh</span>

python3 manage.py makemigrations
python3 manage.py migrate
python3 manage.py createsuperuser <span class="nt">--noinput</span>
python3 manage.py collectstatic <span class="nt">--noinput</span>
python3 manage.py runserver 0.0.0.0:8006
</code></pre></div></div>

<p>Creamos la imagen de Docker:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> legnakra/django:v1 <span class="nb">.</span>
</code></pre></div></div>

<p><img src="/assets/images/docker/p2/1.png" alt="1" /></p>

<p>Ejecutando <code class="language-plaintext highlighter-rouge">docker run -d --name polls --network django-net -p 8080:8006 legnakra/django:v1</code> podemos ver que la aplicación se ejecuta correctamente.</p>

<p><img src="/assets/images/docker/p2/2.png" alt="2" /></p>

<p>Tras ver como en la imagen anterior, todo funciona correctamente, nos disponemos a crear el docker-compose que levantará los dos contenedores que hemos creado.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi docker-compose.yml
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3.7'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">django-tutorial</span><span class="pi">:</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">django-tutorial</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">legnakra/django:v1</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">ALLOWED_HOSTS</span><span class="pi">:</span> <span class="s2">"</span><span class="s">*"</span>
      <span class="na">HOST</span><span class="pi">:</span> <span class="s">bd_mariadb_django</span>
      <span class="na">USUARIO</span><span class="pi">:</span> <span class="s">django</span>
      <span class="na">CONTRA</span><span class="pi">:</span> <span class="s">django</span>
      <span class="na">BASE_DATOS</span><span class="pi">:</span> <span class="s">django</span>
      <span class="na">DJANGO_SUPERUSER_PASSWORD</span><span class="pi">:</span> <span class="s">admin</span>
      <span class="na">DJANGO_SUPERUSER_USERNAME</span><span class="pi">:</span> <span class="s">admin</span>
      <span class="na">DJANGO_SUPERUSER_EMAIL</span><span class="pi">:</span> <span class="s">admin@admin.org</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8084:8006</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">db_django</span>
  <span class="na">db_django</span><span class="pi">:</span>
    <span class="na">container_name</span><span class="pi">:</span> <span class="s">bd_mariadb_django</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">mariadb:latest</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">MARIADB_ROOT_PASSWORD</span><span class="pi">:</span> <span class="s">root</span>
      <span class="na">MARIADB_DATABASE</span><span class="pi">:</span> <span class="s">django</span>
      <span class="na">MARIADB_USER</span><span class="pi">:</span> <span class="s">django</span>
      <span class="na">MARIADB_PASSWORD</span><span class="pi">:</span> <span class="s">django</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">mariadb_data_django:/var/lib/mysql</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">net-mariatec</span>
<span class="na">volumes</span><span class="pi">:</span>
    <span class="na">mariadb_data_django</span><span class="pi">:</span>
<span class="na">netwoks</span><span class="pi">:</span>
  <span class="na">net-mariatec</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">net-mariatec</span>
    <span class="na">external</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p>Levantamos el escenario de nuestro entorno de desarrollo con <code class="language-plaintext highlighter-rouge">docker-compose up -d</code> y podemos comprobar que todo funciona correctamente.</p>

<p><img src="/assets/images/docker/p2/3.png" alt="3" /></p>

<p><img src="/assets/images/docker/p2/4.png" alt="4" /></p>

<p>Nos dirigimos a la zona de administración con las credenciales que hemos configurado en el fichero <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> y podemos ver que accedemos y podemos crear nuevas encuestas.</p>

<p><img src="/assets/images/docker/p2/5.png" alt="5" /></p>

<h2 id="entorno-de-producción">Entorno de producción</h2>

<p>Ya tenemos la aplicación funcionando en nuestro entorno de desarrollo, por lo que ahora solo queda que nos vayamos al entorno de producción y despleguemos la aplicación.</p>

<p>Para ello, nos vamos a crear un registro en nuestro servidor DNS para que podamos acceder a la aplicación desde el exterior.</p>

<p>⬜️ django.mariatec.es        CNAME    mariatec.es</p>

<p>Generamos el certificado SSL para poder acceder a la aplicación desde el exterior.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>certbot certonly <span class="nt">--standalone</span> <span class="nt">-d</span> django.mariatec.com
</code></pre></div></div>

<p><img src="/assets/images/docker/p2/6.png" alt="6" /></p>

<p>Tras generarlos, creamos el proxy de nginx para poder acceder a la aplicación desde el exterior.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vi /etc/nginx/sites-available/django.mariatec.es
</code></pre></div></div>

<p>recuerda que debe ser escuchado por elpuerto 8084.</p>

<div class="language-nginx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">server</span> <span class="p">{</span>
        <span class="kn">listen</span> <span class="mi">80</span><span class="p">;</span>
        <span class="kn">listen</span> <span class="s">[::]:80</span><span class="p">;</span>

        <span class="kn">server_name</span> <span class="s">django.mariatec.es</span><span class="p">;</span>

        <span class="kn">return</span> <span class="mi">301</span> <span class="s">https://</span><span class="nv">$host$request_uri</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">server</span> <span class="p">{</span>
        <span class="kn">listen</span> <span class="mi">443</span> <span class="s">ssl</span> <span class="s">http2</span><span class="p">;</span>
        <span class="kn">listen</span> <span class="s">[::]:443</span> <span class="s">ssl</span> <span class="s">http2</span><span class="p">;</span>

        <span class="kn">ssl</span>    <span class="no">on</span><span class="p">;</span>
        <span class="kn">ssl_certificate</span> <span class="n">/etc/letsencrypt/live/django.mariatec.es/fullchain.pem</span><span class="p">;</span>
        <span class="kn">ssl_certificate_key</span> <span class="n">/etc/letsencrypt/live/django.mariatec.es/privkey.pem</span><span class="p">;</span>

        <span class="kn">index</span> <span class="s">index.html</span> <span class="s">index.php</span> <span class="s">index.htm</span> <span class="s">index.nginx-debian.html</span><span class="p">;</span>

        <span class="kn">server_name</span> <span class="s">django.mariatec.es</span><span class="p">;</span>

        <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
                <span class="kn">proxy_pass</span> <span class="s">http://localhost:8084</span><span class="p">;</span>
                <span class="kn">include</span> <span class="s">proxy_params</span><span class="p">;</span>
        <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Creamos el enlace simbólico para que nginx pueda cargar la configuración y reniciamos el servicio de nginx.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ln</span> <span class="nt">-s</span> /etc/nginx/sites-available/django /etc/nginx/sites-enabled/django

systemctl restart nginx
</code></pre></div></div>

<p>Si accedemos desde nuestro navegador a la dirección <code class="language-plaintext highlighter-rouge">https://django.mariatec.es</code> podemos ver que accedemos a la aplicación.</p>

<p><img src="/assets/images/docker/p2/7.png" alt="7" /></p>

<p>Y que podemos acceder a la zona de administración.</p>

<p><img src="/assets/images/docker/p2/8.png" alt="8" /></p>

<p><img src="/assets/images/docker/p2/9.png" alt="9" /></p>

<p><img src="/assets/images/docker/p2/10.png" alt="10" /></p>

<p><img src="/assets/images/docker/p2/11.png" alt="11" /></p>]]></content><author><name></name></author><category term="IAW" /><summary type="html"><![CDATA[Introducción En este post vamos a configurar contenedores de Docker para ejecutar aplicaciones web Python. Para ello, vamos a utilizar una aplicación web Python sencilla que se encuentra en el repositorio de GitHub. Entorno de desarrollo Vamos a crear un entorno de desarrollo en el que realizaremos las pruebas necesarias para la puesta a punto antes de lanzar nuestra aplicación en producción. También vamos a crear dos contenedores que estarán conectados a la misma red. Una vez comprobado que la aplicación funciona, pasaremos a crear el docker-compose para pasarlo a producción. Creación de la red Para crear la red, ejecutamos el siguiente comando: docker network create net_django Creación del contenedor de la base de datos Ya creada la red en la que van a ir conectados los contenedores, nos ponemos manos a la obra y creamos el contenedor de la base de datos. Para ello, ejecutamos el siguiente comando: docker run -d --name mariadb -v vol_polls:/var/lib/mysql --network net_django -e MARIADB_ROOT_PASSWORD=admin -e MARIADB_USER=django -e MARIADB_PASSWORD=admin -e MARIADB_DATABASE=django mariadb Tendremos que modificar el fichero settings.py de la aplicación web para que sea capaz de leer las variables de entorno: import os BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': os.environ.get("BASE_DATOS"), 'USER': os.environ.get('USUARIO'), 'PASSWORD': os.environ.get("CONTRA"), 'HOST': os.environ.get('HOST'), 'PORT': '3306', } } ALLOWED_HOSTS = [os.environ.get("ALLOWED_HOSTS")] STATIC_ROOT = os.path.join(BASE_DIR, 'static') STATIC_URL = '/static/' CSRF_TRUSTED_ORIGINS = ['http://*.mariatec.es','http://*.127.0.0.1','https://*.mariatec.es','https://*.127.0.0.1'] La estructura del directorio de la aplicación web es la siguiente: ├── practica2 │   ├── django_tutorial │   │   ├── manage.py │   │   ├── requirements.txt │   ├── docker-compose.yml │   ├── Dockerfile │   ├── polls.sh Creamos el fichero Dockerfile a partir de la imagen de Python: FROM python:3 WORKDIR /usr/src/app MAINTAINER Maria Jesús Alloza Rodríguez 'mariajesus.allozarodriguez@gmail.com' RUN apt-get install git &amp;&amp; pip install --root-user-action=ignore --upgrade pip &amp;&amp; pip install --root-user-action=ignore django mysqlclient RUN git clone https://github.com/Legnakra/django_tutorial.git /usr/src/app &amp;&amp; mkdir static ADD ./polls.sh /usr/src/app/ RUN chmod +x /usr/src/app/polls.sh ENV ALLOWED_HOSTS='*' ENV HOST=mariadb ENV USUARIO=django ENV CONTRA=django ENV BASE_DATOS=django ENV DJANGO_SUPERUSER_PASSWORD=admin ENV DJANGO_SUPERUSER_USERNAME=admin ENV DJANGO_SUPERUSER_EMAIL=admin@example.org ENTRYPOINT ["/usr/src/app/polls.sh"] Una vez terminado de crear el fichero Dockerfile, creamos el fichero django_polls.sh: #! /bin/sh python3 manage.py makemigrations python3 manage.py migrate python3 manage.py createsuperuser --noinput python3 manage.py collectstatic --noinput python3 manage.py runserver 0.0.0.0:8006 Creamos la imagen de Docker: docker build -t legnakra/django:v1 . Ejecutando docker run -d --name polls --network django-net -p 8080:8006 legnakra/django:v1 podemos ver que la aplicación se ejecuta correctamente. Tras ver como en la imagen anterior, todo funciona correctamente, nos disponemos a crear el docker-compose que levantará los dos contenedores que hemos creado. vi docker-compose.yml version: '3.7' services: django-tutorial: container_name: django-tutorial image: legnakra/django:v1 restart: always environment: ALLOWED_HOSTS: "*" HOST: bd_mariadb_django USUARIO: django CONTRA: django BASE_DATOS: django DJANGO_SUPERUSER_PASSWORD: admin DJANGO_SUPERUSER_USERNAME: admin DJANGO_SUPERUSER_EMAIL: admin@admin.org ports: - 8084:8006 depends_on: - db_django db_django: container_name: bd_mariadb_django image: mariadb:latest restart: always environment: MARIADB_ROOT_PASSWORD: root MARIADB_DATABASE: django MARIADB_USER: django MARIADB_PASSWORD: django volumes: - mariadb_data_django:/var/lib/mysql networks: - net-mariatec volumes: mariadb_data_django: netwoks: net-mariatec: name: net-mariatec external: true Levantamos el escenario de nuestro entorno de desarrollo con docker-compose up -d y podemos comprobar que todo funciona correctamente. Nos dirigimos a la zona de administración con las credenciales que hemos configurado en el fichero docker-compose.yml y podemos ver que accedemos y podemos crear nuevas encuestas. Entorno de producción Ya tenemos la aplicación funcionando en nuestro entorno de desarrollo, por lo que ahora solo queda que nos vayamos al entorno de producción y despleguemos la aplicación. Para ello, nos vamos a crear un registro en nuestro servidor DNS para que podamos acceder a la aplicación desde el exterior. ⬜️ django.mariatec.es CNAME mariatec.es Generamos el certificado SSL para poder acceder a la aplicación desde el exterior. certbot certonly --standalone -d django.mariatec.com Tras generarlos, creamos el proxy de nginx para poder acceder a la aplicación desde el exterior. vi /etc/nginx/sites-available/django.mariatec.es recuerda que debe ser escuchado por elpuerto 8084. server { listen 80; listen [::]:80; server_name django.mariatec.es; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; ssl on; ssl_certificate /etc/letsencrypt/live/django.mariatec.es/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/django.mariatec.es/privkey.pem; index index.html index.php index.htm index.nginx-debian.html; server_name django.mariatec.es; location / { proxy_pass http://localhost:8084; include proxy_params; } } Creamos el enlace simbólico para que nginx pueda cargar la configuración y reniciamos el servicio de nginx. ln -s /etc/nginx/sites-available/django /etc/nginx/sites-enabled/django systemctl restart nginx Si accedemos desde nuestro navegador a la dirección https://django.mariatec.es podemos ver que accedemos a la aplicación. Y que podemos acceder a la zona de administración.]]></summary></entry><entry><title type="html">Taller Kubernetes: Almacenamiento en Kubernetes</title><link href="/hlc+sri/2023/02/10/kubernetes-almacenamiento.html" rel="alternate" type="text/html" title="Taller Kubernetes: Almacenamiento en Kubernetes" /><published>2023-02-10T16:45:16+01:00</published><updated>2023-02-10T16:45:16+01:00</updated><id>/hlc+sri/2023/02/10/kubernetes-almacenamiento</id><content type="html" xml:base="/hlc+sri/2023/02/10/kubernetes-almacenamiento.html"><![CDATA[<h2 id="introducción">Introducción</h2>

<p>En este taller, vamos a ver cómo Kubernetes gestiona el almacenamiento de los contenedores. Para ello, vamos a trabajar con recursos de almacenamiento que se pueden crear en Kubernetes. Estos recursos son PersistentVolume y PersistentVolumeClaim.</p>

<h2 id="ejercicio-1-desplegando-un-servidor-web-persistente">Ejercicio 1: Desplegando un servidor web persistente</h2>

<p>En este ejercicio, vamos a desplegar un servidor web que guarde los datos en un volumen persistente. Para ello, vamos a crear un PersistentVolume y un PersistentVolumeClaim. El PersistentVolumeClaim se va a asociar al PersistentVolume para que el servidor web pueda acceder a él.</p>

<ol>
  <li>
    <p>Creamos el fichero que va a definir el PersistentVolume. El fichero es el siguiente:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">pvc-serverweb</span>
 <span class="na">spec</span><span class="pi">:</span>
     <span class="na">accessModes</span><span class="pi">:</span>
     <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
     <span class="na">resources</span><span class="pi">:</span>
       <span class="na">requests</span><span class="pi">:</span>
         <span class="na">storage</span><span class="pi">:</span> <span class="s">2Gi</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el PersistentVolume con el siguiente comando:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> pvc-serverweb.yaml
</code></pre></div>    </div>

    <p>Y comprobamos que se ha creado correctamente:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get pc, pvc
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/1.png" alt="1" /></p>
  </li>
  <li>
    <p>Creamos el fichero de despliegue del servidor web. El fichero es el siguiente:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">web-php</span>
   <span class="na">labels</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">apache</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
   <span class="na">selector</span><span class="pi">:</span>
     <span class="na">matchLabels</span><span class="pi">:</span>
       <span class="na">app</span><span class="pi">:</span> <span class="s">apache</span>
   <span class="na">template</span><span class="pi">:</span>
     <span class="na">metadata</span><span class="pi">:</span>
       <span class="na">labels</span><span class="pi">:</span>
         <span class="na">app</span><span class="pi">:</span> <span class="s">apache</span>
     <span class="na">spec</span><span class="pi">:</span>
       <span class="na">volumes</span><span class="pi">:</span>
         <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-web-php</span>
           <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
             <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc-webserver</span>
       <span class="na">containers</span><span class="pi">:</span>
         <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-apache-php</span>
           <span class="na">image</span><span class="pi">:</span> <span class="s">php:7.4-apache</span>
           <span class="na">ports</span><span class="pi">:</span>
             <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http-server</span>
               <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
           <span class="na">volumeMounts</span><span class="pi">:</span>
             <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/www/html"</span>
               <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-web-php</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el despliegue con el siguiente comando:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> web-php.yaml
</code></pre></div>    </div>

    <p>Y comprobamos que se ha creado correctamente:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get all
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/2.png" alt="2" /></p>
  </li>
  <li>
    <p>Creamos el fichero de servicio del servidor web. El fichero es el siguiente:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">servicio-web-php</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
   <span class="na">ports</span><span class="pi">:</span>
   <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">service-http</span>
     <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
     <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http-server</span>
   <span class="na">selector</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">apache</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el servicio con el siguiente comando:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> servicio-web-php.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/3.png" alt="3" /></p>
  </li>
  <li>
    <p>Creamos el fichero <code class="language-plaintext highlighter-rouge">info.php</code> que vamos a guardar en el volumen persistente. Para ello, primero deberemos averiguar la ID del pod:</p>

    <p>En la imagen del punto anterior, podemos ver que el pod se llama <code class="language-plaintext highlighter-rouge">pod/servidorweb-778bc767f5-dvf7x</code></p>

    <p>Sabido esto, crearemos el fichero <code class="language-plaintext highlighter-rouge">info.php</code> con el siguiente comando:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl <span class="nb">exec </span>pod/servidorweb-778bc767f5-dvf7x <span class="nt">--</span> bash <span class="nt">-c</span> <span class="s2">"echo '&lt;?php phpinfo(); ?&gt;' &gt; /var/www/html/info.php"</span>
</code></pre></div>    </div>

    <p>Para comprobar que se ha creado correctamente, abriremos el fichero php en el navegador:</p>

    <p><img src="/assets/images/kubernetes/t6/4.png" alt="4" /></p>
  </li>
  <li>
    <p>Como último paso, vamos a comprobar la persistencia, y por ello, vamos a eliminar el despliegue y lo volvemos a crear:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl delete deployment.apps/servidorweb
 kubectl apply <span class="nt">-f</span> web-php.yaml
</code></pre></div>    </div>

    <p>Y comprobamos que se ha creado correctamente:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get all
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/5.png" alt="5" /></p>

    <p>Y comprobamos que el fichero <code class="language-plaintext highlighter-rouge">info.php</code> sigue estando:</p>

    <p><img src="/assets/images/kubernetes/t6/6.png" alt="6" /></p>
  </li>
</ol>

<h2 id="ejercicio-2-haciendo-persistente-la-aplicación-guestbook">Ejercicio 2: Haciendo persistente la aplicación GuestBook</h2>

<p>En este ejercicio vamos a desplegar nuestra aplicació Guestbook y vamos a hacer persistente la base de datos.</p>

<ol>
  <li>
    <p>Creamos el fichero con el que vamos a definir el volumen <code class="language-plaintext highlighter-rouge">PersistentVolumenClaim</code>:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
 <span class="na">metadata</span><span class="pi">:</span>
     <span class="na">name</span><span class="pi">:</span> <span class="s">pvc-redis</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">accessModes</span><span class="pi">:</span>
     <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
   <span class="na">resources</span><span class="pi">:</span>
     <span class="na">requests</span><span class="pi">:</span>
       <span class="na">storage</span><span class="pi">:</span> <span class="s">3Gi</span>
</code></pre></div>    </div>

    <p>Lo aplicamos y comprobamos que se ha creado correctamente:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> pvc-redis.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/7.png" alt="7" /></p>
  </li>
  <li>
    <p>Creamos el fichero de despliegue de la base de datos:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">redis</span>
   <span class="na">labels</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">redis</span>
     <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
   <span class="na">selector</span><span class="pi">:</span>
     <span class="na">matchLabels</span><span class="pi">:</span>
       <span class="na">app</span><span class="pi">:</span> <span class="s">redis</span>
       <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
   <span class="na">template</span><span class="pi">:</span>
     <span class="na">metadata</span><span class="pi">:</span>
       <span class="na">labels</span><span class="pi">:</span>
         <span class="na">app</span><span class="pi">:</span> <span class="s">redis</span>
         <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
     <span class="na">spec</span><span class="pi">:</span>
       <span class="na">volumes</span><span class="pi">:</span>
         <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-redis</span>
           <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
             <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc-redis</span>
       <span class="na">containers</span><span class="pi">:</span>
         <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-redis</span>
           <span class="na">image</span><span class="pi">:</span> <span class="s">redis</span>
           <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">redis-server"</span><span class="pi">]</span>
           <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">--appendonly"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">yes"</span><span class="pi">]</span>
           <span class="na">ports</span><span class="pi">:</span>
             <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">redis-server</span>
               <span class="na">containerPort</span><span class="pi">:</span> <span class="m">6379</span>
           <span class="na">volumeMounts</span><span class="pi">:</span>
             <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/data"</span>
               <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-redis</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el despliegue de guestbook:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">guestbook</span>
   <span class="na">labels</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">guestbook</span>
     <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
   <span class="na">selector</span><span class="pi">:</span>
     <span class="na">matchLabels</span><span class="pi">:</span>
       <span class="na">app</span><span class="pi">:</span> <span class="s">guestbook</span>
       <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
   <span class="na">template</span><span class="pi">:</span>
     <span class="na">metadata</span><span class="pi">:</span>
       <span class="na">labels</span><span class="pi">:</span>
         <span class="na">app</span><span class="pi">:</span> <span class="s">guestbook</span>
         <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
     <span class="na">spec</span><span class="pi">:</span>
       <span class="na">containers</span><span class="pi">:</span>
       <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-guestbook</span>
         <span class="na">image</span><span class="pi">:</span> <span class="s">iesgn/guestbook</span>
         <span class="na">ports</span><span class="pi">:</span>
           <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http-server</span>
             <span class="na">containerPort</span><span class="pi">:</span> <span class="m">5000</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el servicio de guestbook:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">guestbook</span>
   <span class="na">labels</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">guestbook</span>
     <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
   <span class="na">ports</span><span class="pi">:</span>
   <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
     <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http-server</span>
   <span class="na">selector</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">guestbook</span>
     <span class="na">tier</span><span class="pi">:</span> <span class="s">frontend</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el servicio de redis:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
 <span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
 <span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">redis</span>
   <span class="na">labels</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">redis</span>
     <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
 <span class="na">spec</span><span class="pi">:</span>
   <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
   <span class="na">ports</span><span class="pi">:</span>
   <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">6379</span>
     <span class="na">targetPort</span><span class="pi">:</span> <span class="s">redis-server</span>
   <span class="na">selector</span><span class="pi">:</span>
     <span class="na">app</span><span class="pi">:</span> <span class="s">redis</span>
     <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Realizamos el despliegue de los servicios y comprobamos que se han creado correctamente:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl apply <span class="nt">-f</span> <span class="nb">.</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/8.png" alt="8" /></p>
  </li>
  <li>
    <p>Comprobamos que la aplicación funciona correctamente:</p>

    <p><img src="/assets/images/kubernetes/t6/9.png" alt="9" /></p>
  </li>
  <li>
    <p>Eliminamos el despliegue de guestbook y lo volvemos a crear:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl delete <span class="nt">-f</span> redis-deployment.yaml
 kubectl apply <span class="nt">-f</span> redis-deployment.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/10.png" alt="10" /></p>
  </li>
  <li>
    <p>Accedemos a la aplicación y comprobamos que los datos siguen estando:</p>

    <p><img src="/assets/images/kubernetes/t6/11.png" alt="11" /></p>
  </li>
</ol>

<h2 id="ejercicio-3-haciendo-persistente-la-aplicación-nextcloud">Ejercicio 3: Haciendo persistente la aplicación Nextcloud</h2>

<p>En este ejercicio vamos a desplegar nuestra aplicación Nextcloud y vamos a hacer persistente tanto la aplicación como la base de datos.</p>

<ul>
  <li>
    <p>Creamos los volúmenes: 1 para la base de datos y otro para la aplicación, ambos de 4GB.</p>

    <ul>
      <li>
        <p>Volumen para la base de datos:</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
  <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">pvc-mariadb</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">accessModes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">storage</span><span class="pi">:</span> <span class="s">4Gi</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Volumen para la aplicación:</p>

        <p><code class="language-plaintext highlighter-rouge">yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: pvc-nextcloud
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 4Gi
     </code></p>
      </li>
    </ul>

    <p>Y los aplicamos:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  kubectl apply <span class="nt">-f</span> <span class="nb">.</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/12.png" alt="12" /></p>
  </li>
  <li>
    <p>Creamos el recurso Secret, ejecuta el siguiente comando:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  kubectl create cm bd-datos <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">bd_user</span><span class="o">=</span>nextcloud <span class="se">\</span>
  <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">bd_dbname</span><span class="o">=</span>nextcloud <span class="nt">-o</span> yaml <span class="nt">--dry-run</span><span class="o">=</span>client <span class="o">&gt;</span> bd_datos_configmap.yaml

  kubectl create secret generic bd-passwords <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">bd_password</span><span class="o">=</span>nextcloud <span class="se">\</span>
  <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">bd_rootpassword</span><span class="o">=</span>nextcloud <span class="nt">-o</span> yaml <span class="nt">--dry-run</span><span class="o">=</span>client <span class="o">&gt;</span> bd_passwords_secret.yaml
</code></pre></div>    </div>

    <p>Y los desplegamos:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  kubectl apply <span class="nt">-f</span> bd_datos_configmap.yaml
  kubectl apply <span class="nt">-f</span> bd_passwords_secret.yaml
</code></pre></div>    </div>

    <p><img src="/assets/images/kubernetes/t6/13.png" alt="13" /></p>
  </li>
  <li>
    <p>Creamos el fichero de despliegue de la base de datos.</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mariadb-deployment</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">database</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">database</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">database</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-mariadb</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">mariadb:10.5</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">3306</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">db-port</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_USER</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">configMapKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_user</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_DATABASE</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">configMapKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_dbname</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_PASSWORD</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">secretKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_password</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_ROOT_PASSWORD</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">secretKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_root_password</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-mariadb</span>
        <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
          <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc-mariadb</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Creamos el servicio de la base de datos:</p>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mariadb-service</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">database</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">database</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">3306</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="s">db-port</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
</code></pre></div></div>

<ul>
  <li>Creamos el despliegue de Nextcloud:</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nextcloud-deployment</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">frontend</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">frontend</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">contenedor-nextcloud</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">nextcloud:latest</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">http-port</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">443</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">https-port</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_HOST</span>
              <span class="na">value</span><span class="pi">:</span> <span class="s">mariadb-service</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_USER</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">configMapKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_user</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_PASSWORD</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">secretKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_password</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MYSQL_DATABASE</span>
              <span class="na">valueFrom</span><span class="pi">:</span>
                <span class="na">configMapKeyRef</span><span class="pi">:</span>
                  <span class="na">name</span><span class="pi">:</span> <span class="s">bd-datos</span>
                  <span class="na">key</span><span class="pi">:</span> <span class="s">bd_dbname</span>
        <span class="na">volumes</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">volumen-nextcloud</span>
          <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
            <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc-nextcloud</span>
</code></pre></div></div>

<ul>
  <li>Creamos el servicio de Nextcloud:</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nextcloud-service</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">frontend</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">frontend</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http-sv-port</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http-port</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">https-sv-port</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="s">https-port</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
</code></pre></div></div>

<ul>
  <li>Creamos el fichero Ingress para Nextcloud:</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">nextcloud-ingress</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">nextcloud</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">frontend</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">www.maria-nextcloud.org</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">nextcloud-service</span>
              <span class="s">port</span><span class="err">:</span>
                <span class="na">number</span><span class="pi">:</span> <span class="s">http-sv-port</span>
</code></pre></div></div>

<ul>
  <li>Desplegamos los recursos:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> <span class="nb">.</span>
</code></pre></div></div>

<p><img src="/assets/images/kubernetes/t6/14.png" alt="14" /></p>

<ul>
  <li>Cambiamos el fichero <code class="language-plaintext highlighter-rouge">/etc/hosts</code> para que apunte a nuestro cluster:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nano /etc/hosts
</code></pre></div></div>

<p><img src="/assets/images/kubernetes/t6/15.png" alt="15" /></p>

<ul>
  <li>Accedemos a Nextcloud:</li>
</ul>

<p>Entramos en el navegador y accedemos a la dirección <code class="language-plaintext highlighter-rouge">www.maria-nextcloud.org</code>:</p>

<p><img src="/assets/images/kubernetes/t6/16.png" alt="16" /></p>

<p>Y nos logueamos con las siguientes credenciales:</p>

<ul>
  <li>Usuario: <code class="language-plaintext highlighter-rouge">nextcloud</code></li>
  <li>Contraseña: <code class="language-plaintext highlighter-rouge">bmV4dGNsb3Vk</code></li>
</ul>

<p><img src="/assets/images/kubernetes/t6/17.png" alt="17" /></p>]]></content><author><name></name></author><category term="HLC+SRI" /><summary type="html"><![CDATA[Introducción]]></summary></entry></feed>